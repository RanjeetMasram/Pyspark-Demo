{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intellesence in Jupyter Notebook\n",
    "%config IPCompleter.greedy=True\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pyspark library\n",
    "import findspark\n",
    "\n",
    "# specifying the path of SPARK_HOME\n",
    "findspark.init(r\"D:\\Study\\PySpark\\Pysparksetup\\spark\")\n",
    "\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()\n",
    "# importing SparkSession to create Spark session\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pyspark' from 'D:\\\\Study\\\\PySpark\\\\Pysparksetup\\\\spark\\\\python\\\\pyspark\\\\__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the shell, a SparkContext is automatically created for you as the variable called \"sc\".\n",
    "# On typing \"sc\" in pyspark console, you would get the following message, if the SparkContext is created\n",
    "# while executing on notebook we have to create spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Spark Session\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"LogisticRegWithSpark\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://RanjeetPC:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>LogisticRegWithSpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1e9a4138780>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris1 = spark.sparkContext.textFile(r\"dataset/iris/iris_site.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://RanjeetPC:4041'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Web-URL with which we can check its status\n",
    "spark.sparkContext.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[2] at readRDDFromFile at PythonRDD.scala:262\n"
     ]
    }
   ],
   "source": [
    "data1 = spark.sparkContext.parallelize([\"Hello\", \"World\"])\n",
    "print(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.1,3.5,1.4,0.2,setosa'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print firt line in RDD\n",
    "iris1.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5.1,3.5,1.4,0.2,setosa',\n",
       " '4.9,3.0,1.4,0.2,setosa',\n",
       " '4.7,3.2,1.3,0.2,setosa',\n",
       " '4.6,3.1,1.5,0.2,setosa',\n",
       " '5.0,3.6,1.4,0.2,setosa',\n",
       " '5.4,3.9,1.7,0.4,setosa',\n",
       " '4.6,3.4,1.4,0.3,setosa',\n",
       " '5.0,3.4,1.5,0.2,setosa',\n",
       " '4.4,2.9,1.4,0.2,setosa',\n",
       " '4.9,3.1,1.5,0.1,setosa',\n",
       " '5.4,3.7,1.5,0.2,setosa',\n",
       " '4.8,3.4,1.6,0.2,setosa',\n",
       " '4.8,3.0,1.4,0.1,setosa',\n",
       " '4.3,3.0,1.1,0.1,setosa',\n",
       " '5.8,4.0,1.2,0.2,setosa',\n",
       " '5.7,4.4,1.5,0.4,setosa',\n",
       " '5.4,3.9,1.3,0.4,setosa',\n",
       " '5.1,3.5,1.4,0.3,setosa',\n",
       " '5.7,3.8,1.7,0.3,setosa',\n",
       " '5.1,3.8,1.5,0.3,setosa',\n",
       " '5.4,3.4,1.7,0.2,setosa',\n",
       " '5.1,3.7,1.5,0.4,setosa',\n",
       " '4.6,3.6,1.0,0.2,setosa',\n",
       " '5.1,3.3,1.7,0.5,setosa',\n",
       " '4.8,3.4,1.9,0.2,setosa',\n",
       " '5.0,3.0,1.6,0.2,setosa',\n",
       " '5.0,3.4,1.6,0.4,setosa',\n",
       " '5.2,3.5,1.5,0.2,setosa',\n",
       " '5.2,3.4,1.4,0.2,setosa',\n",
       " '4.7,3.2,1.6,0.2,setosa',\n",
       " '4.8,3.1,1.6,0.2,setosa',\n",
       " '5.4,3.4,1.5,0.4,setosa',\n",
       " '5.2,4.1,1.5,0.1,setosa',\n",
       " '5.5,4.2,1.4,0.2,setosa',\n",
       " '4.9,3.1,1.5,0.2,setosa',\n",
       " '5.0,3.2,1.2,0.2,setosa',\n",
       " '5.5,3.5,1.3,0.2,setosa',\n",
       " '4.9,3.6,1.4,0.1,setosa',\n",
       " '4.4,3.0,1.3,0.2,setosa',\n",
       " '5.1,3.4,1.5,0.2,setosa',\n",
       " '5.0,3.5,1.3,0.3,setosa',\n",
       " '4.5,2.3,1.3,0.3,setosa',\n",
       " '4.4,3.2,1.3,0.2,setosa',\n",
       " '5.0,3.5,1.6,0.6,setosa',\n",
       " '5.1,3.8,1.9,0.4,setosa',\n",
       " '4.8,3.0,1.4,0.3,setosa',\n",
       " '5.1,3.8,1.6,0.2,setosa',\n",
       " '4.6,3.2,1.4,0.2,setosa',\n",
       " '5.3,3.7,1.5,0.2,setosa',\n",
       " '5.0,3.3,1.4,0.2,setosa',\n",
       " '7.0,3.2,4.7,1.4,versicolor',\n",
       " '6.4,3.2,4.5,1.5,versicolor',\n",
       " '6.9,3.1,4.9,1.5,versicolor',\n",
       " '5.5,2.3,4.0,1.3,versicolor',\n",
       " '6.5,2.8,4.6,1.5,versicolor',\n",
       " '5.7,2.8,4.5,1.3,versicolor',\n",
       " '6.3,3.3,4.7,1.6,versicolor',\n",
       " '4.9,2.4,3.3,1.0,versicolor',\n",
       " '6.6,2.9,4.6,1.3,versicolor',\n",
       " '5.2,2.7,3.9,1.4,versicolor',\n",
       " '5.0,2.0,3.5,1.0,versicolor',\n",
       " '5.9,3.0,4.2,1.5,versicolor',\n",
       " '6.0,2.2,4.0,1.0,versicolor',\n",
       " '6.1,2.9,4.7,1.4,versicolor',\n",
       " '5.6,2.9,3.6,1.3,versicolor',\n",
       " '6.7,3.1,4.4,1.4,versicolor',\n",
       " '5.6,3.0,4.5,1.5,versicolor',\n",
       " '5.8,2.7,4.1,1.0,versicolor',\n",
       " '6.2,2.2,4.5,1.5,versicolor',\n",
       " '5.6,2.5,3.9,1.1,versicolor',\n",
       " '5.9,3.2,4.8,1.8,versicolor',\n",
       " '6.1,2.8,4.0,1.3,versicolor',\n",
       " '6.3,2.5,4.9,1.5,versicolor',\n",
       " '6.1,2.8,4.7,1.2,versicolor',\n",
       " '6.4,2.9,4.3,1.3,versicolor',\n",
       " '6.6,3.0,4.4,1.4,versicolor',\n",
       " '6.8,2.8,4.8,1.4,versicolor',\n",
       " '6.7,3.0,5.0,1.7,versicolor',\n",
       " '6.0,2.9,4.5,1.5,versicolor',\n",
       " '5.7,2.6,3.5,1.0,versicolor',\n",
       " '5.5,2.4,3.8,1.1,versicolor',\n",
       " '5.5,2.4,3.7,1.0,versicolor',\n",
       " '5.8,2.7,3.9,1.2,versicolor',\n",
       " '6.0,2.7,5.1,1.6,versicolor',\n",
       " '5.4,3.0,4.5,1.5,versicolor',\n",
       " '6.0,3.4,4.5,1.6,versicolor',\n",
       " '6.7,3.1,4.7,1.5,versicolor',\n",
       " '6.3,2.3,4.4,1.3,versicolor',\n",
       " '5.6,3.0,4.1,1.3,versicolor',\n",
       " '5.5,2.5,4.0,1.3,versicolor',\n",
       " '5.5,2.6,4.4,1.2,versicolor',\n",
       " '6.1,3.0,4.6,1.4,versicolor',\n",
       " '5.8,2.6,4.0,1.2,versicolor',\n",
       " '5.0,2.3,3.3,1.0,versicolor',\n",
       " '5.6,2.7,4.2,1.3,versicolor',\n",
       " '5.7,3.0,4.2,1.2,versicolor',\n",
       " '5.7,2.9,4.2,1.3,versicolor',\n",
       " '6.2,2.9,4.3,1.3,versicolor',\n",
       " '5.1,2.5,3.0,1.1,versicolor',\n",
       " '5.7,2.8,4.1,1.3,versicolor',\n",
       " '6.3,3.3,6.0,2.5,virginica',\n",
       " '5.8,2.7,5.1,1.9,virginica',\n",
       " '7.1,3.0,5.9,2.1,virginica',\n",
       " '6.3,2.9,5.6,1.8,virginica',\n",
       " '6.5,3.0,5.8,2.2,virginica',\n",
       " '7.6,3.0,6.6,2.1,virginica',\n",
       " '4.9,2.5,4.5,1.7,virginica',\n",
       " '7.3,2.9,6.3,1.8,virginica',\n",
       " '6.7,2.5,5.8,1.8,virginica',\n",
       " '7.2,3.6,6.1,2.5,virginica',\n",
       " '6.5,3.2,5.1,2.0,virginica',\n",
       " '6.4,2.7,5.3,1.9,virginica',\n",
       " '6.8,3.0,5.5,2.1,virginica',\n",
       " '5.7,2.5,5.0,2.0,virginica',\n",
       " '5.8,2.8,5.1,2.4,virginica',\n",
       " '6.4,3.2,5.3,2.3,virginica',\n",
       " '6.5,3.0,5.5,1.8,virginica',\n",
       " '7.7,3.8,6.7,2.2,virginica',\n",
       " '7.7,2.6,6.9,2.3,virginica',\n",
       " '6.0,2.2,5.0,1.5,virginica',\n",
       " '6.9,3.2,5.7,2.3,virginica',\n",
       " '5.6,2.8,4.9,2.0,virginica',\n",
       " '7.7,2.8,6.7,2.0,virginica',\n",
       " '6.3,2.7,4.9,1.8,virginica',\n",
       " '6.7,3.3,5.7,2.1,virginica',\n",
       " '7.2,3.2,6.0,1.8,virginica',\n",
       " '6.2,2.8,4.8,1.8,virginica',\n",
       " '6.1,3.0,4.9,1.8,virginica',\n",
       " '6.4,2.8,5.6,2.1,virginica',\n",
       " '7.2,3.0,5.8,1.6,virginica',\n",
       " '7.4,2.8,6.1,1.9,virginica',\n",
       " '7.9,3.8,6.4,2.0,virginica',\n",
       " '6.4,2.8,5.6,2.2,virginica',\n",
       " '6.3,2.8,5.1,1.5,virginica',\n",
       " '6.1,2.6,5.6,1.4,virginica',\n",
       " '7.7,3.0,6.1,2.3,virginica',\n",
       " '6.3,3.4,5.6,2.4,virginica',\n",
       " '6.4,3.1,5.5,1.8,virginica',\n",
       " '6.0,3.0,4.8,1.8,virginica',\n",
       " '6.9,3.1,5.4,2.1,virginica',\n",
       " '6.7,3.1,5.6,2.4,virginica',\n",
       " '6.9,3.1,5.1,2.3,virginica',\n",
       " '5.8,2.7,5.1,1.9,virginica',\n",
       " '6.8,3.2,5.9,2.3,virginica',\n",
       " '6.7,3.3,5.7,2.5,virginica',\n",
       " '6.7,3.0,5.2,2.3,virginica',\n",
       " '6.3,2.5,5.0,1.9,virginica',\n",
       " '6.5,3.0,5.2,2.0,virginica',\n",
       " '6.2,3.4,5.4,2.3,virginica',\n",
       " '5.9,3.0,5.1,1.8,virginica']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all lines of RDD\n",
    "iris1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/iris/iris_site.csv\n",
      "import_RD\n"
     ]
    }
   ],
   "source": [
    "# RDD Meta Data\n",
    "\n",
    "print(iris1.name())\n",
    "\n",
    "# Name of an RDD can be changed using the 'setName' command. \n",
    "iris1.setName('import_RD')\n",
    "print(iris1.name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any transformation on an RDD creates a new RDD. \n",
    "# The result of all the transformation can be stored in a persistent storage\n",
    "# in the spark server using 'saveAsTextFile' command.\n",
    "iris1 = spark.sparkContext.textFile(r\"dataset/iris/iris_site.csv\")\n",
    "#iris1.saveAsTextFile(\"./out1.csv\")  # works on Hadoop System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting RDD to Python Dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[6] at readRDDFromFile at PythonRDD.scala:262\n",
      "[('Sepal.Length', [5.1, 4.9, 4.7, 4.6]), ('Sepal.Width', [3.5, 3.0, 3.2, 3.1]), ('Species', ['setosa', 'setosa', 'setosa', 'setosa'])]\n",
      "**************************************************\n",
      "{'Sepal.Length': [5.1, 4.9, 4.7, 4.6], 'Sepal.Width': [3.5, 3.0, 3.2, 3.1], 'Species': ['setosa', 'setosa', 'setosa', 'setosa']}\n",
      "[5.1, 4.9, 4.7, 4.6]\n",
      "[3.5, 3.0, 3.2, 3.1]\n",
      "['setosa', 'setosa', 'setosa', 'setosa']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "There are situations when the data is required to be present as a python object\n",
    "for performing some operations on the same. So, if a data is present as an RDD, \n",
    "then it can be converted to python dictionary object using 'collectAsMap' function.\n",
    "\n",
    "In this example, we first create an RDD with the help of 'sc.parallelize' function. \n",
    "It is to be noted that the RDD contains data in the form of a key-value pair where the\n",
    "key is the name of the column, and value is the list of values present in that column.\n",
    "'''\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize([\n",
    "    ('Sepal.Length', [5.1,4.9,4.7,4.6]),\n",
    "    ('Sepal.Width', [3.5,3.0,3.2,3.1]),\n",
    "    ('Species', ['setosa','setosa','setosa','setosa']) \n",
    "])\n",
    "print(rdd1)\n",
    "print(rdd1.collect())\n",
    "\n",
    "'''\n",
    "The created RDD is now converted into python dictionary using 'collectAsMap' function.\n",
    "''' \n",
    "print(\"*\"* 50)\n",
    "dict1 = rdd1.collectAsMap()\n",
    "print(dict1)\n",
    "print(dict1['Sepal.Length'])\n",
    "print(dict1['Sepal.Width'])\n",
    "print(dict1['Species'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Structured Data using RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data Based on Delimiter\n",
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "#print(iris1_split.take(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5.1,3.5,1.4,0.2,setosa',\n",
       " '4.9,3.0,1.4,0.2,setosa',\n",
       " '4.7,3.2,1.3,0.2,setosa',\n",
       " '4.6,3.1,1.5,0.2,setosa',\n",
       " '5.0,3.6,1.4,0.2,setosa',\n",
       " '5.4,3.9,1.7,0.4,setosa',\n",
       " '4.6,3.4,1.4,0.3,setosa',\n",
       " '5.0,3.4,1.5,0.2,setosa',\n",
       " '4.4,2.9,1.4,0.2,setosa',\n",
       " '4.9,3.1,1.5,0.1,setosa',\n",
       " '5.4,3.7,1.5,0.2,setosa',\n",
       " '4.8,3.4,1.6,0.2,setosa',\n",
       " '4.8,3.0,1.4,0.1,setosa',\n",
       " '4.3,3.0,1.1,0.1,setosa',\n",
       " '5.8,4.0,1.2,0.2,setosa',\n",
       " '5.7,4.4,1.5,0.4,setosa',\n",
       " '5.4,3.9,1.3,0.4,setosa',\n",
       " '5.1,3.5,1.4,0.3,setosa',\n",
       " '5.7,3.8,1.7,0.3,setosa',\n",
       " '5.1,3.8,1.5,0.3,setosa',\n",
       " '5.4,3.4,1.7,0.2,setosa',\n",
       " '5.1,3.7,1.5,0.4,setosa',\n",
       " '4.6,3.6,1.0,0.2,setosa',\n",
       " '5.1,3.3,1.7,0.5,setosa',\n",
       " '4.8,3.4,1.9,0.2,setosa',\n",
       " '5.0,3.0,1.6,0.2,setosa',\n",
       " '5.0,3.4,1.6,0.4,setosa',\n",
       " '5.2,3.5,1.5,0.2,setosa',\n",
       " '5.2,3.4,1.4,0.2,setosa',\n",
       " '4.7,3.2,1.6,0.2,setosa',\n",
       " '4.8,3.1,1.6,0.2,setosa',\n",
       " '5.4,3.4,1.5,0.4,setosa',\n",
       " '5.2,4.1,1.5,0.1,setosa',\n",
       " '5.5,4.2,1.4,0.2,setosa',\n",
       " '4.9,3.1,1.5,0.2,setosa',\n",
       " '5.0,3.2,1.2,0.2,setosa',\n",
       " '5.5,3.5,1.3,0.2,setosa',\n",
       " '4.9,3.6,1.4,0.1,setosa',\n",
       " '4.4,3.0,1.3,0.2,setosa',\n",
       " '5.1,3.4,1.5,0.2,setosa',\n",
       " '5.0,3.5,1.3,0.3,setosa',\n",
       " '4.5,2.3,1.3,0.3,setosa',\n",
       " '4.4,3.2,1.3,0.2,setosa',\n",
       " '5.0,3.5,1.6,0.6,setosa',\n",
       " '5.1,3.8,1.9,0.4,setosa',\n",
       " '4.8,3.0,1.4,0.3,setosa',\n",
       " '5.1,3.8,1.6,0.2,setosa',\n",
       " '4.6,3.2,1.4,0.2,setosa',\n",
       " '5.3,3.7,1.5,0.2,setosa',\n",
       " '5.0,3.3,1.4,0.2,setosa',\n",
       " '7.0,3.2,4.7,1.4,versicolor',\n",
       " '6.4,3.2,4.5,1.5,versicolor',\n",
       " '6.9,3.1,4.9,1.5,versicolor',\n",
       " '5.5,2.3,4.0,1.3,versicolor',\n",
       " '6.5,2.8,4.6,1.5,versicolor',\n",
       " '5.7,2.8,4.5,1.3,versicolor',\n",
       " '6.3,3.3,4.7,1.6,versicolor',\n",
       " '4.9,2.4,3.3,1.0,versicolor',\n",
       " '6.6,2.9,4.6,1.3,versicolor',\n",
       " '5.2,2.7,3.9,1.4,versicolor',\n",
       " '5.0,2.0,3.5,1.0,versicolor',\n",
       " '5.9,3.0,4.2,1.5,versicolor',\n",
       " '6.0,2.2,4.0,1.0,versicolor',\n",
       " '6.1,2.9,4.7,1.4,versicolor',\n",
       " '5.6,2.9,3.6,1.3,versicolor',\n",
       " '6.7,3.1,4.4,1.4,versicolor',\n",
       " '5.6,3.0,4.5,1.5,versicolor',\n",
       " '5.8,2.7,4.1,1.0,versicolor',\n",
       " '6.2,2.2,4.5,1.5,versicolor',\n",
       " '5.6,2.5,3.9,1.1,versicolor',\n",
       " '5.9,3.2,4.8,1.8,versicolor',\n",
       " '6.1,2.8,4.0,1.3,versicolor',\n",
       " '6.3,2.5,4.9,1.5,versicolor',\n",
       " '6.1,2.8,4.7,1.2,versicolor',\n",
       " '6.4,2.9,4.3,1.3,versicolor',\n",
       " '6.6,3.0,4.4,1.4,versicolor',\n",
       " '6.8,2.8,4.8,1.4,versicolor',\n",
       " '6.7,3.0,5.0,1.7,versicolor',\n",
       " '6.0,2.9,4.5,1.5,versicolor',\n",
       " '5.7,2.6,3.5,1.0,versicolor',\n",
       " '5.5,2.4,3.8,1.1,versicolor',\n",
       " '5.5,2.4,3.7,1.0,versicolor',\n",
       " '5.8,2.7,3.9,1.2,versicolor',\n",
       " '6.0,2.7,5.1,1.6,versicolor',\n",
       " '5.4,3.0,4.5,1.5,versicolor',\n",
       " '6.0,3.4,4.5,1.6,versicolor',\n",
       " '6.7,3.1,4.7,1.5,versicolor',\n",
       " '6.3,2.3,4.4,1.3,versicolor',\n",
       " '5.6,3.0,4.1,1.3,versicolor',\n",
       " '5.5,2.5,4.0,1.3,versicolor',\n",
       " '5.5,2.6,4.4,1.2,versicolor',\n",
       " '6.1,3.0,4.6,1.4,versicolor',\n",
       " '5.8,2.6,4.0,1.2,versicolor',\n",
       " '5.0,2.3,3.3,1.0,versicolor',\n",
       " '5.6,2.7,4.2,1.3,versicolor',\n",
       " '5.7,3.0,4.2,1.2,versicolor',\n",
       " '5.7,2.9,4.2,1.3,versicolor',\n",
       " '6.2,2.9,4.3,1.3,versicolor',\n",
       " '5.1,2.5,3.0,1.1,versicolor',\n",
       " '5.7,2.8,4.1,1.3,versicolor',\n",
       " '6.3,3.3,6.0,2.5,virginica',\n",
       " '5.8,2.7,5.1,1.9,virginica',\n",
       " '7.1,3.0,5.9,2.1,virginica',\n",
       " '6.3,2.9,5.6,1.8,virginica',\n",
       " '6.5,3.0,5.8,2.2,virginica',\n",
       " '7.6,3.0,6.6,2.1,virginica',\n",
       " '4.9,2.5,4.5,1.7,virginica',\n",
       " '7.3,2.9,6.3,1.8,virginica',\n",
       " '6.7,2.5,5.8,1.8,virginica',\n",
       " '7.2,3.6,6.1,2.5,virginica',\n",
       " '6.5,3.2,5.1,2.0,virginica',\n",
       " '6.4,2.7,5.3,1.9,virginica',\n",
       " '6.8,3.0,5.5,2.1,virginica',\n",
       " '5.7,2.5,5.0,2.0,virginica',\n",
       " '5.8,2.8,5.1,2.4,virginica',\n",
       " '6.4,3.2,5.3,2.3,virginica',\n",
       " '6.5,3.0,5.5,1.8,virginica',\n",
       " '7.7,3.8,6.7,2.2,virginica',\n",
       " '7.7,2.6,6.9,2.3,virginica',\n",
       " '6.0,2.2,5.0,1.5,virginica',\n",
       " '6.9,3.2,5.7,2.3,virginica',\n",
       " '5.6,2.8,4.9,2.0,virginica',\n",
       " '7.7,2.8,6.7,2.0,virginica',\n",
       " '6.3,2.7,4.9,1.8,virginica',\n",
       " '6.7,3.3,5.7,2.1,virginica',\n",
       " '7.2,3.2,6.0,1.8,virginica',\n",
       " '6.2,2.8,4.8,1.8,virginica',\n",
       " '6.1,3.0,4.9,1.8,virginica',\n",
       " '6.4,2.8,5.6,2.1,virginica',\n",
       " '7.2,3.0,5.8,1.6,virginica',\n",
       " '7.4,2.8,6.1,1.9,virginica',\n",
       " '7.9,3.8,6.4,2.0,virginica',\n",
       " '6.4,2.8,5.6,2.2,virginica',\n",
       " '6.3,2.8,5.1,1.5,virginica',\n",
       " '6.1,2.6,5.6,1.4,virginica',\n",
       " '7.7,3.0,6.1,2.3,virginica',\n",
       " '6.3,3.4,5.6,2.4,virginica',\n",
       " '6.4,3.1,5.5,1.8,virginica',\n",
       " '6.0,3.0,4.8,1.8,virginica',\n",
       " '6.9,3.1,5.4,2.1,virginica',\n",
       " '6.7,3.1,5.6,2.4,virginica',\n",
       " '6.9,3.1,5.1,2.3,virginica',\n",
       " '5.8,2.7,5.1,1.9,virginica',\n",
       " '6.8,3.2,5.9,2.3,virginica',\n",
       " '6.7,3.3,5.7,2.5,virginica',\n",
       " '6.7,3.0,5.2,2.3,virginica',\n",
       " '6.3,2.5,5.0,1.9,virginica',\n",
       " '6.5,3.0,5.2,2.0,virginica',\n",
       " '6.2,3.4,5.4,2.3,virginica',\n",
       " '5.9,3.0,5.1,1.8,virginica']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['5.1', '3.5', '1.4', '0.2', 'setosa'],\n",
       " ['4.9', '3.0', '1.4', '0.2', 'setosa'],\n",
       " ['4.7', '3.2', '1.3', '0.2', 'setosa'],\n",
       " ['4.6', '3.1', '1.5', '0.2', 'setosa'],\n",
       " ['5.0', '3.6', '1.4', '0.2', 'setosa'],\n",
       " ['5.4', '3.9', '1.7', '0.4', 'setosa'],\n",
       " ['4.6', '3.4', '1.4', '0.3', 'setosa'],\n",
       " ['5.0', '3.4', '1.5', '0.2', 'setosa'],\n",
       " ['4.4', '2.9', '1.4', '0.2', 'setosa'],\n",
       " ['4.9', '3.1', '1.5', '0.1', 'setosa'],\n",
       " ['5.4', '3.7', '1.5', '0.2', 'setosa'],\n",
       " ['4.8', '3.4', '1.6', '0.2', 'setosa'],\n",
       " ['4.8', '3.0', '1.4', '0.1', 'setosa'],\n",
       " ['4.3', '3.0', '1.1', '0.1', 'setosa'],\n",
       " ['5.8', '4.0', '1.2', '0.2', 'setosa'],\n",
       " ['5.7', '4.4', '1.5', '0.4', 'setosa'],\n",
       " ['5.4', '3.9', '1.3', '0.4', 'setosa'],\n",
       " ['5.1', '3.5', '1.4', '0.3', 'setosa'],\n",
       " ['5.7', '3.8', '1.7', '0.3', 'setosa'],\n",
       " ['5.1', '3.8', '1.5', '0.3', 'setosa'],\n",
       " ['5.4', '3.4', '1.7', '0.2', 'setosa'],\n",
       " ['5.1', '3.7', '1.5', '0.4', 'setosa'],\n",
       " ['4.6', '3.6', '1.0', '0.2', 'setosa'],\n",
       " ['5.1', '3.3', '1.7', '0.5', 'setosa'],\n",
       " ['4.8', '3.4', '1.9', '0.2', 'setosa'],\n",
       " ['5.0', '3.0', '1.6', '0.2', 'setosa'],\n",
       " ['5.0', '3.4', '1.6', '0.4', 'setosa'],\n",
       " ['5.2', '3.5', '1.5', '0.2', 'setosa'],\n",
       " ['5.2', '3.4', '1.4', '0.2', 'setosa'],\n",
       " ['4.7', '3.2', '1.6', '0.2', 'setosa'],\n",
       " ['4.8', '3.1', '1.6', '0.2', 'setosa'],\n",
       " ['5.4', '3.4', '1.5', '0.4', 'setosa'],\n",
       " ['5.2', '4.1', '1.5', '0.1', 'setosa'],\n",
       " ['5.5', '4.2', '1.4', '0.2', 'setosa'],\n",
       " ['4.9', '3.1', '1.5', '0.2', 'setosa'],\n",
       " ['5.0', '3.2', '1.2', '0.2', 'setosa'],\n",
       " ['5.5', '3.5', '1.3', '0.2', 'setosa'],\n",
       " ['4.9', '3.6', '1.4', '0.1', 'setosa'],\n",
       " ['4.4', '3.0', '1.3', '0.2', 'setosa'],\n",
       " ['5.1', '3.4', '1.5', '0.2', 'setosa'],\n",
       " ['5.0', '3.5', '1.3', '0.3', 'setosa'],\n",
       " ['4.5', '2.3', '1.3', '0.3', 'setosa'],\n",
       " ['4.4', '3.2', '1.3', '0.2', 'setosa'],\n",
       " ['5.0', '3.5', '1.6', '0.6', 'setosa'],\n",
       " ['5.1', '3.8', '1.9', '0.4', 'setosa'],\n",
       " ['4.8', '3.0', '1.4', '0.3', 'setosa'],\n",
       " ['5.1', '3.8', '1.6', '0.2', 'setosa'],\n",
       " ['4.6', '3.2', '1.4', '0.2', 'setosa'],\n",
       " ['5.3', '3.7', '1.5', '0.2', 'setosa'],\n",
       " ['5.0', '3.3', '1.4', '0.2', 'setosa'],\n",
       " ['7.0', '3.2', '4.7', '1.4', 'versicolor'],\n",
       " ['6.4', '3.2', '4.5', '1.5', 'versicolor'],\n",
       " ['6.9', '3.1', '4.9', '1.5', 'versicolor'],\n",
       " ['5.5', '2.3', '4.0', '1.3', 'versicolor'],\n",
       " ['6.5', '2.8', '4.6', '1.5', 'versicolor'],\n",
       " ['5.7', '2.8', '4.5', '1.3', 'versicolor'],\n",
       " ['6.3', '3.3', '4.7', '1.6', 'versicolor'],\n",
       " ['4.9', '2.4', '3.3', '1.0', 'versicolor'],\n",
       " ['6.6', '2.9', '4.6', '1.3', 'versicolor'],\n",
       " ['5.2', '2.7', '3.9', '1.4', 'versicolor'],\n",
       " ['5.0', '2.0', '3.5', '1.0', 'versicolor'],\n",
       " ['5.9', '3.0', '4.2', '1.5', 'versicolor'],\n",
       " ['6.0', '2.2', '4.0', '1.0', 'versicolor'],\n",
       " ['6.1', '2.9', '4.7', '1.4', 'versicolor'],\n",
       " ['5.6', '2.9', '3.6', '1.3', 'versicolor'],\n",
       " ['6.7', '3.1', '4.4', '1.4', 'versicolor'],\n",
       " ['5.6', '3.0', '4.5', '1.5', 'versicolor'],\n",
       " ['5.8', '2.7', '4.1', '1.0', 'versicolor'],\n",
       " ['6.2', '2.2', '4.5', '1.5', 'versicolor'],\n",
       " ['5.6', '2.5', '3.9', '1.1', 'versicolor'],\n",
       " ['5.9', '3.2', '4.8', '1.8', 'versicolor'],\n",
       " ['6.1', '2.8', '4.0', '1.3', 'versicolor'],\n",
       " ['6.3', '2.5', '4.9', '1.5', 'versicolor'],\n",
       " ['6.1', '2.8', '4.7', '1.2', 'versicolor'],\n",
       " ['6.4', '2.9', '4.3', '1.3', 'versicolor'],\n",
       " ['6.6', '3.0', '4.4', '1.4', 'versicolor'],\n",
       " ['6.8', '2.8', '4.8', '1.4', 'versicolor'],\n",
       " ['6.7', '3.0', '5.0', '1.7', 'versicolor'],\n",
       " ['6.0', '2.9', '4.5', '1.5', 'versicolor'],\n",
       " ['5.7', '2.6', '3.5', '1.0', 'versicolor'],\n",
       " ['5.5', '2.4', '3.8', '1.1', 'versicolor'],\n",
       " ['5.5', '2.4', '3.7', '1.0', 'versicolor'],\n",
       " ['5.8', '2.7', '3.9', '1.2', 'versicolor'],\n",
       " ['6.0', '2.7', '5.1', '1.6', 'versicolor'],\n",
       " ['5.4', '3.0', '4.5', '1.5', 'versicolor'],\n",
       " ['6.0', '3.4', '4.5', '1.6', 'versicolor'],\n",
       " ['6.7', '3.1', '4.7', '1.5', 'versicolor'],\n",
       " ['6.3', '2.3', '4.4', '1.3', 'versicolor'],\n",
       " ['5.6', '3.0', '4.1', '1.3', 'versicolor'],\n",
       " ['5.5', '2.5', '4.0', '1.3', 'versicolor'],\n",
       " ['5.5', '2.6', '4.4', '1.2', 'versicolor'],\n",
       " ['6.1', '3.0', '4.6', '1.4', 'versicolor'],\n",
       " ['5.8', '2.6', '4.0', '1.2', 'versicolor'],\n",
       " ['5.0', '2.3', '3.3', '1.0', 'versicolor'],\n",
       " ['5.6', '2.7', '4.2', '1.3', 'versicolor'],\n",
       " ['5.7', '3.0', '4.2', '1.2', 'versicolor'],\n",
       " ['5.7', '2.9', '4.2', '1.3', 'versicolor'],\n",
       " ['6.2', '2.9', '4.3', '1.3', 'versicolor'],\n",
       " ['5.1', '2.5', '3.0', '1.1', 'versicolor'],\n",
       " ['5.7', '2.8', '4.1', '1.3', 'versicolor'],\n",
       " ['6.3', '3.3', '6.0', '2.5', 'virginica'],\n",
       " ['5.8', '2.7', '5.1', '1.9', 'virginica'],\n",
       " ['7.1', '3.0', '5.9', '2.1', 'virginica'],\n",
       " ['6.3', '2.9', '5.6', '1.8', 'virginica'],\n",
       " ['6.5', '3.0', '5.8', '2.2', 'virginica'],\n",
       " ['7.6', '3.0', '6.6', '2.1', 'virginica'],\n",
       " ['4.9', '2.5', '4.5', '1.7', 'virginica'],\n",
       " ['7.3', '2.9', '6.3', '1.8', 'virginica'],\n",
       " ['6.7', '2.5', '5.8', '1.8', 'virginica'],\n",
       " ['7.2', '3.6', '6.1', '2.5', 'virginica'],\n",
       " ['6.5', '3.2', '5.1', '2.0', 'virginica'],\n",
       " ['6.4', '2.7', '5.3', '1.9', 'virginica'],\n",
       " ['6.8', '3.0', '5.5', '2.1', 'virginica'],\n",
       " ['5.7', '2.5', '5.0', '2.0', 'virginica'],\n",
       " ['5.8', '2.8', '5.1', '2.4', 'virginica'],\n",
       " ['6.4', '3.2', '5.3', '2.3', 'virginica'],\n",
       " ['6.5', '3.0', '5.5', '1.8', 'virginica'],\n",
       " ['7.7', '3.8', '6.7', '2.2', 'virginica'],\n",
       " ['7.7', '2.6', '6.9', '2.3', 'virginica'],\n",
       " ['6.0', '2.2', '5.0', '1.5', 'virginica'],\n",
       " ['6.9', '3.2', '5.7', '2.3', 'virginica'],\n",
       " ['5.6', '2.8', '4.9', '2.0', 'virginica'],\n",
       " ['7.7', '2.8', '6.7', '2.0', 'virginica'],\n",
       " ['6.3', '2.7', '4.9', '1.8', 'virginica'],\n",
       " ['6.7', '3.3', '5.7', '2.1', 'virginica'],\n",
       " ['7.2', '3.2', '6.0', '1.8', 'virginica'],\n",
       " ['6.2', '2.8', '4.8', '1.8', 'virginica'],\n",
       " ['6.1', '3.0', '4.9', '1.8', 'virginica'],\n",
       " ['6.4', '2.8', '5.6', '2.1', 'virginica'],\n",
       " ['7.2', '3.0', '5.8', '1.6', 'virginica'],\n",
       " ['7.4', '2.8', '6.1', '1.9', 'virginica'],\n",
       " ['7.9', '3.8', '6.4', '2.0', 'virginica'],\n",
       " ['6.4', '2.8', '5.6', '2.2', 'virginica'],\n",
       " ['6.3', '2.8', '5.1', '1.5', 'virginica'],\n",
       " ['6.1', '2.6', '5.6', '1.4', 'virginica'],\n",
       " ['7.7', '3.0', '6.1', '2.3', 'virginica'],\n",
       " ['6.3', '3.4', '5.6', '2.4', 'virginica'],\n",
       " ['6.4', '3.1', '5.5', '1.8', 'virginica'],\n",
       " ['6.0', '3.0', '4.8', '1.8', 'virginica'],\n",
       " ['6.9', '3.1', '5.4', '2.1', 'virginica'],\n",
       " ['6.7', '3.1', '5.6', '2.4', 'virginica'],\n",
       " ['6.9', '3.1', '5.1', '2.3', 'virginica'],\n",
       " ['5.8', '2.7', '5.1', '1.9', 'virginica'],\n",
       " ['6.8', '3.2', '5.9', '2.3', 'virginica'],\n",
       " ['6.7', '3.3', '5.7', '2.5', 'virginica'],\n",
       " ['6.7', '3.0', '5.2', '2.3', 'virginica'],\n",
       " ['6.3', '2.5', '5.0', '1.9', 'virginica'],\n",
       " ['6.5', '3.0', '5.2', '2.0', 'virginica'],\n",
       " ['6.2', '3.4', '5.4', '2.3', 'virginica'],\n",
       " ['5.9', '3.0', '5.1', '1.8', 'virginica']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1_split.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['5.1', '3.5', '1.4', '0.2', 'setosa'], ['4.9', '3.0', '1.4', '0.2', 'setosa'], ['4.7', '3.2', '1.3', '0.2', 'setosa'], ['4.6', '3.1', '1.5', '0.2', 'setosa'], ['5.0', '3.6', '1.4', '0.2', 'setosa'], ['5.4', '3.9', '1.7', '0.4', 'setosa'], ['4.6', '3.4', '1.4', '0.3', 'setosa'], ['5.0', '3.4', '1.5', '0.2', 'setosa'], ['4.4', '2.9', '1.4', '0.2', 'setosa'], ['4.9', '3.1', '1.5', '0.1', 'setosa']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['5.1', '3.5', '1.4', '0.2', 'setosa'],\n",
       " ['4.9', '3.0', '1.4', '0.2', 'setosa'],\n",
       " ['4.7', '3.2', '1.3', '0.2', 'setosa'],\n",
       " ['4.6', '3.1', '1.5', '0.2', 'setosa'],\n",
       " ['5.0', '3.6', '1.4', '0.2', 'setosa'],\n",
       " ['5.4', '3.9', '1.7', '0.4', 'setosa'],\n",
       " ['4.6', '3.4', '1.4', '0.3', 'setosa'],\n",
       " ['5.0', '3.4', '1.5', '0.2', 'setosa'],\n",
       " ['4.4', '2.9', '1.4', '0.2', 'setosa'],\n",
       " ['4.9', '3.1', '1.5', '0.1', 'setosa']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Different was to perform same task\n",
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "print(iris1_split.take(10))\n",
    "\n",
    "# Chaining Spark Commands\n",
    "spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\").map(lambda var1: var1.split(\",\")).take(10)\n",
    "\n",
    "#Defining Transformation Function\n",
    "def fun1(var1):\n",
    "    return var1.split(\",\")\n",
    "spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\").map(fun1).take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n",
      "['setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa']\n"
     ]
    }
   ],
   "source": [
    "# Extracting a Particular Column\n",
    "\n",
    "# In Python\n",
    "l1 = ['a','b','c','d']\n",
    "print(l1[1])\n",
    "\n",
    "# In Pyspark\n",
    "\n",
    "'''\n",
    "So, in our case with iris data set, we can extract the second element of all\n",
    "the rows by using python indexing inside 'map' function. The map function is\n",
    "applied on each input value. Here, each input value is a collection (python list).\n",
    "So, by applying indexing inside map function, the element with that particular\n",
    "index will be extracted from each element of the collection (python list/tuple).\n",
    "\n",
    "Once splitted, any particular column can be extracted using indexing.\n",
    "'''\n",
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "iris1_mod = iris1_split.map(lambda col:col[4]) #5th Column Data\n",
    "print(iris1_mod.take(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type Casting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3.5', '3.0', '3.2', '3.1', '3.6', '3.9', '3.4', '3.4', '2.9', '3.1']\n",
      "**************************************************\n",
      "[3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.4, 3.0, 3.0, 4.0, 4.4, 3.9, 3.5, 3.8, 3.8, 3.4, 3.7, 3.6, 3.3, 3.4, 3.0, 3.4, 3.5, 3.4, 3.2, 3.1, 3.4, 4.1, 4.2, 3.1, 3.2, 3.5, 3.6, 3.0, 3.4, 3.5, 2.3, 3.2, 3.5, 3.8, 3.0, 3.8, 3.2, 3.7, 3.3, 3.2, 3.2, 3.1, 2.3, 2.8, 2.8, 3.3, 2.4, 2.9, 2.7, 2.0, 3.0, 2.2, 2.9, 2.9, 3.1, 3.0, 2.7, 2.2, 2.5, 3.2, 2.8, 2.5, 2.8, 2.9, 3.0, 2.8, 3.0, 2.9, 2.6, 2.4, 2.4, 2.7, 2.7, 3.0, 3.4, 3.1, 2.3, 3.0, 2.5, 2.6, 3.0, 2.6, 2.3, 2.7, 3.0, 2.9, 2.9, 2.5, 2.8, 3.3, 2.7, 3.0, 2.9, 3.0, 3.0, 2.5, 2.9, 2.5, 3.6, 3.2, 2.7, 3.0, 2.5, 2.8, 3.2, 3.0, 3.8, 2.6, 2.2, 3.2, 2.8, 2.8, 2.7, 3.3, 3.2, 2.8, 3.0, 2.8, 3.0, 2.8, 3.8, 2.8, 2.8, 2.6, 3.0, 3.4, 3.1, 3.0, 3.1, 3.1, 3.1, 2.7, 3.2, 3.3, 3.0, 2.5, 3.0, 3.4, 3.0]\n"
     ]
    }
   ],
   "source": [
    "# Type Casting Single Column\n",
    "\n",
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "print(iris1_split.map(lambda col:col[1]).take(10))\n",
    "\n",
    "# It can be observed from the result that all the data is present as a string\n",
    "print(\"*\"*50)\n",
    "\n",
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "print(iris1_split.map(lambda col:float(col[1])).collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1, 3.5, 1.4, 0.2, 'setosa'], [4.9, 3.0, 1.4, 0.2, 'setosa'], [4.7, 3.2, 1.3, 0.2, 'setosa'], [4.6, 3.1, 1.5, 0.2, 'setosa'], [5.0, 3.6, 1.4, 0.2, 'setosa'], [5.4, 3.9, 1.7, 0.4, 'setosa'], [4.6, 3.4, 1.4, 0.3, 'setosa'], [5.0, 3.4, 1.5, 0.2, 'setosa'], [4.4, 2.9, 1.4, 0.2, 'setosa'], [4.9, 3.1, 1.5, 0.1, 'setosa']]\n"
     ]
    }
   ],
   "source": [
    "# Type Casting All Columns Values\n",
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "iris1_split_mod = iris1_split.map(lambda var1: [float(var1[0]), float(var1[1]), float(var1[2]), float(var1[3]), var1[4]])\n",
    "print(iris1_split_mod.take(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python For Loop with RDD Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "3.2 <class 'float'>\n",
      "3.1 <class 'float'>\n",
      "3.6 <class 'float'>\n",
      "3.9 <class 'float'>\n",
      "3.4 <class 'float'>\n",
      "3.4 <class 'float'>\n",
      "2.9 <class 'float'>\n",
      "3.1 <class 'float'>\n",
      "3.7 <class 'float'>\n",
      "3.4 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "4.0 <class 'float'>\n",
      "4.4 <class 'float'>\n",
      "3.9 <class 'float'>\n",
      "3.5 <class 'float'>\n",
      "3.8 <class 'float'>\n",
      "3.8 <class 'float'>\n",
      "3.4 <class 'float'>\n",
      "3.7 <class 'float'>\n",
      "3.6 <class 'float'>\n",
      "3.3 <class 'float'>\n",
      "3.4 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "3.4 <class 'float'>\n",
      "3.5 <class 'float'>\n",
      "3.4 <class 'float'>\n",
      "3.2 <class 'float'>\n",
      "3.1 <class 'float'>\n",
      "3.4 <class 'float'>\n",
      "4.1 <class 'float'>\n",
      "4.2 <class 'float'>\n",
      "3.1 <class 'float'>\n",
      "3.2 <class 'float'>\n",
      "3.5 <class 'float'>\n",
      "3.6 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "3.4 <class 'float'>\n",
      "3.5 <class 'float'>\n",
      "2.3 <class 'float'>\n",
      "3.2 <class 'float'>\n",
      "3.5 <class 'float'>\n",
      "3.8 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "3.8 <class 'float'>\n",
      "3.2 <class 'float'>\n",
      "3.7 <class 'float'>\n",
      "3.3 <class 'float'>\n",
      "3.2 <class 'float'>\n",
      "3.2 <class 'float'>\n",
      "3.1 <class 'float'>\n",
      "2.3 <class 'float'>\n",
      "2.8 <class 'float'>\n",
      "2.8 <class 'float'>\n",
      "3.3 <class 'float'>\n",
      "2.4 <class 'float'>\n",
      "2.9 <class 'float'>\n",
      "2.7 <class 'float'>\n",
      "2.0 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "2.2 <class 'float'>\n",
      "2.9 <class 'float'>\n",
      "2.9 <class 'float'>\n",
      "3.1 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "2.7 <class 'float'>\n",
      "2.2 <class 'float'>\n",
      "2.5 <class 'float'>\n",
      "3.2 <class 'float'>\n",
      "2.8 <class 'float'>\n",
      "2.5 <class 'float'>\n",
      "2.8 <class 'float'>\n",
      "2.9 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "2.8 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "2.9 <class 'float'>\n",
      "2.6 <class 'float'>\n",
      "2.4 <class 'float'>\n",
      "2.4 <class 'float'>\n",
      "2.7 <class 'float'>\n",
      "2.7 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "3.4 <class 'float'>\n",
      "3.1 <class 'float'>\n",
      "2.3 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "2.5 <class 'float'>\n",
      "2.6 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "2.6 <class 'float'>\n",
      "2.3 <class 'float'>\n",
      "2.7 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "2.9 <class 'float'>\n",
      "2.9 <class 'float'>\n",
      "2.5 <class 'float'>\n",
      "2.8 <class 'float'>\n",
      "3.3 <class 'float'>\n",
      "2.7 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "2.9 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "2.5 <class 'float'>\n",
      "2.9 <class 'float'>\n",
      "2.5 <class 'float'>\n",
      "3.6 <class 'float'>\n",
      "3.2 <class 'float'>\n",
      "2.7 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "2.5 <class 'float'>\n",
      "2.8 <class 'float'>\n",
      "3.2 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "3.8 <class 'float'>\n",
      "2.6 <class 'float'>\n",
      "2.2 <class 'float'>\n",
      "3.2 <class 'float'>\n",
      "2.8 <class 'float'>\n",
      "2.8 <class 'float'>\n",
      "2.7 <class 'float'>\n",
      "3.3 <class 'float'>\n",
      "3.2 <class 'float'>\n",
      "2.8 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "2.8 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "2.8 <class 'float'>\n",
      "3.8 <class 'float'>\n",
      "2.8 <class 'float'>\n",
      "2.8 <class 'float'>\n",
      "2.6 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "3.4 <class 'float'>\n",
      "3.1 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "3.1 <class 'float'>\n",
      "3.1 <class 'float'>\n",
      "3.1 <class 'float'>\n",
      "2.7 <class 'float'>\n",
      "3.2 <class 'float'>\n",
      "3.3 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "2.5 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "3.4 <class 'float'>\n",
      "3.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "# Python For Loop with RDD Objects\n",
    "\n",
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "for var1 in iris1_split.map(lambda var1: float(var1[1])).collect():\n",
    "    print(var1,type(var1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Key-Value Pairs in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('Sepal.Length', 5.1), ('Sepal.Width', 3.5), ('Petal.Length', 1.4), ('Petal.Width', 0.2)), (('Sepal.Length', 4.9), ('Sepal.Width', 3.0), ('Petal.Length', 1.4), ('Petal.Width', 0.2))]\n"
     ]
    }
   ],
   "source": [
    "# Creating Key-Value Pairs in RDD\n",
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "iris1_mod = iris1_split.map(lambda var1:(('Sepal.Length',float(var1[0])),('Sepal.Width',float(var1[1])),('Petal.Length',float(var1[2])),('Petal.Width',float(var1[3]))))\n",
    "print(iris1_mod.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sepal.Length', 5.1), ('Sepal.Width', 3.5), ('Petal.Length', 1.4), ('Petal.Width', 0.2), ('Sepal.Length', 4.9), ('Sepal.Width', 3.0), ('Petal.Length', 1.4), ('Petal.Width', 0.2), ('Sepal.Length', 4.7), ('Sepal.Width', 3.2)]\n"
     ]
    }
   ],
   "source": [
    "# flatMap()\n",
    "# It can be observed from the result that the extra level of the collection has been removed.\n",
    "#from operator import add\n",
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "iris1_mod = iris1_split.flatMap(lambda var1:(('Sepal.Length',float(var1[0])),('Sepal.Width',float(var1[1])),('Petal.Length',float(var1[2])),('Petal.Width',float(var1[3]))))\n",
    "print(iris1_mod.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['5.1', '3.5', '1.4', '0.2'], ['4.9', '3.0', '1.4', '0.2']]\n",
      "['5.1', '3.5', '1.4', '0.2']\n"
     ]
    }
   ],
   "source": [
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var: var.split(\",\"))\n",
    "iris1_mod = iris1_split.map(lambda var1: [var1[0],var1[1],var1[2],var1[3]])\n",
    "#iris1_mod.take(2)\n",
    "print(iris1_mod.take(2))\n",
    "iris2_mod = iris1_split.flatMap(lambda var1: [var1[0],var1[1],var1[2],var1[3]])\n",
    "print(iris2_mod.take(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4.3, 3.0, 1.1, 0.1, 'setosa'),\n",
       " (4.4, 2.9, 1.4, 0.2, 'setosa'),\n",
       " (4.4, 3.0, 1.3, 0.2, 'setosa'),\n",
       " (4.4, 3.2, 1.3, 0.2, 'setosa'),\n",
       " (4.5, 2.3, 1.3, 0.3, 'setosa'),\n",
       " (4.6, 3.1, 1.5, 0.2, 'setosa'),\n",
       " (4.6, 3.4, 1.4, 0.3, 'setosa'),\n",
       " (4.6, 3.6, 1.0, 0.2, 'setosa'),\n",
       " (4.6, 3.2, 1.4, 0.2, 'setosa'),\n",
       " (4.7, 3.2, 1.3, 0.2, 'setosa')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple Sort\n",
    "# iris data set is sorted based on 'Sepal.Length' column.\n",
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "iris1_mod = iris1_split.map(lambda var1:(float(var1[0]), float(var1[1]), float(var1[2]), float(var1[3]), var1[4]))\n",
    "iris1_mod.sortBy(lambda x: x[0]).take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('setosa', ['5.1', 3.5, 1.4, 0.2]),\n",
       " ('setosa', ['4.9', 3.0, 1.4, 0.2]),\n",
       " ('setosa', ['4.7', 3.2, 1.3, 0.2]),\n",
       " ('setosa', ['4.6', 3.1, 1.5, 0.2]),\n",
       " ('setosa', ['5.0', 3.6, 1.4, 0.2]),\n",
       " ('setosa', ['5.4', 3.9, 1.7, 0.4]),\n",
       " ('setosa', ['4.6', 3.4, 1.4, 0.3]),\n",
       " ('setosa', ['5.0', 3.4, 1.5, 0.2]),\n",
       " ('setosa', ['4.4', 2.9, 1.4, 0.2]),\n",
       " ('setosa', ['4.9', 3.1, 1.5, 0.1]),\n",
       " ('setosa', ['5.4', 3.7, 1.5, 0.2]),\n",
       " ('setosa', ['4.8', 3.4, 1.6, 0.2]),\n",
       " ('setosa', ['4.8', 3.0, 1.4, 0.1]),\n",
       " ('setosa', ['4.3', 3.0, 1.1, 0.1]),\n",
       " ('setosa', ['5.8', 4.0, 1.2, 0.2]),\n",
       " ('setosa', ['5.7', 4.4, 1.5, 0.4]),\n",
       " ('setosa', ['5.4', 3.9, 1.3, 0.4]),\n",
       " ('setosa', ['5.1', 3.5, 1.4, 0.3]),\n",
       " ('setosa', ['5.7', 3.8, 1.7, 0.3]),\n",
       " ('setosa', ['5.1', 3.8, 1.5, 0.3]),\n",
       " ('setosa', ['5.4', 3.4, 1.7, 0.2]),\n",
       " ('setosa', ['5.1', 3.7, 1.5, 0.4]),\n",
       " ('setosa', ['4.6', 3.6, 1.0, 0.2]),\n",
       " ('setosa', ['5.1', 3.3, 1.7, 0.5]),\n",
       " ('setosa', ['4.8', 3.4, 1.9, 0.2]),\n",
       " ('setosa', ['5.0', 3.0, 1.6, 0.2]),\n",
       " ('setosa', ['5.0', 3.4, 1.6, 0.4]),\n",
       " ('setosa', ['5.2', 3.5, 1.5, 0.2]),\n",
       " ('setosa', ['5.2', 3.4, 1.4, 0.2]),\n",
       " ('setosa', ['4.7', 3.2, 1.6, 0.2]),\n",
       " ('setosa', ['4.8', 3.1, 1.6, 0.2]),\n",
       " ('setosa', ['5.4', 3.4, 1.5, 0.4]),\n",
       " ('setosa', ['5.2', 4.1, 1.5, 0.1]),\n",
       " ('setosa', ['5.5', 4.2, 1.4, 0.2]),\n",
       " ('setosa', ['4.9', 3.1, 1.5, 0.2]),\n",
       " ('setosa', ['5.0', 3.2, 1.2, 0.2]),\n",
       " ('setosa', ['5.5', 3.5, 1.3, 0.2]),\n",
       " ('setosa', ['4.9', 3.6, 1.4, 0.1]),\n",
       " ('setosa', ['4.4', 3.0, 1.3, 0.2]),\n",
       " ('setosa', ['5.1', 3.4, 1.5, 0.2]),\n",
       " ('setosa', ['5.0', 3.5, 1.3, 0.3]),\n",
       " ('setosa', ['4.5', 2.3, 1.3, 0.3]),\n",
       " ('setosa', ['4.4', 3.2, 1.3, 0.2]),\n",
       " ('setosa', ['5.0', 3.5, 1.6, 0.6]),\n",
       " ('setosa', ['5.1', 3.8, 1.9, 0.4]),\n",
       " ('setosa', ['4.8', 3.0, 1.4, 0.3]),\n",
       " ('setosa', ['5.1', 3.8, 1.6, 0.2]),\n",
       " ('setosa', ['4.6', 3.2, 1.4, 0.2]),\n",
       " ('setosa', ['5.3', 3.7, 1.5, 0.2]),\n",
       " ('setosa', ['5.0', 3.3, 1.4, 0.2]),\n",
       " ('versicolor', ['7.0', 3.2, 4.7, 1.4]),\n",
       " ('versicolor', ['6.4', 3.2, 4.5, 1.5]),\n",
       " ('versicolor', ['6.9', 3.1, 4.9, 1.5]),\n",
       " ('versicolor', ['5.5', 2.3, 4.0, 1.3]),\n",
       " ('versicolor', ['6.5', 2.8, 4.6, 1.5])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sorting RDD Based on Key\n",
    "# species column has been set as key and all the other columns are set as value.\n",
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "iris1_mod = iris1_split.map(lambda var1:(var1[4], [var1[0],float(var1[1]),float(var1[2]),float(var1[3])]))\n",
    "iris1_mod.sortByKey(ascending=True,keyfunc=lambda k: k).take(55)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Union Multiple RDDs, Intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sepal.Length', 5.1], ['Sepal.Length', 4.9], ['Sepal.Length', 4.7], ['Sepal.Length', 4.6], ['Sepal.Length', 5.0], ['Sepal.Length', 5.4], ['Sepal.Length', 4.6], ['Sepal.Length', 5.0], ['Sepal.Length', 4.4], ['Sepal.Length', 4.9]]\n",
      "**************************************************\n",
      "[['Sepal.Width', 3.5], ['Sepal.Width', 3.0], ['Sepal.Width', 3.2], ['Sepal.Width', 3.1], ['Sepal.Width', 3.6], ['Sepal.Width', 3.9], ['Sepal.Width', 3.4], ['Sepal.Width', 3.4], ['Sepal.Width', 2.9], ['Sepal.Width', 3.1]]\n",
      "**************************************************\n",
      "[['Petal.Length', 1.4], ['Petal.Length', 1.4], ['Petal.Length', 1.3], ['Petal.Length', 1.5], ['Petal.Length', 1.4], ['Petal.Length', 1.7], ['Petal.Length', 1.4], ['Petal.Length', 1.5], ['Petal.Length', 1.4], ['Petal.Length', 1.5]]\n",
      "**************************************************\n",
      "[['Petal.Width', 0.2], ['Petal.Width', 0.2], ['Petal.Width', 0.2], ['Petal.Width', 0.2], ['Petal.Width', 0.2], ['Petal.Width', 0.4], ['Petal.Width', 0.3], ['Petal.Width', 0.2], ['Petal.Width', 0.2], ['Petal.Width', 0.1]]\n",
      "**************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Sepal.Length', 5.1],\n",
       " ['Sepal.Length', 4.9],\n",
       " ['Sepal.Length', 4.7],\n",
       " ['Sepal.Length', 4.6],\n",
       " ['Sepal.Length', 5.0],\n",
       " ['Sepal.Length', 5.4],\n",
       " ['Sepal.Length', 4.6],\n",
       " ['Sepal.Length', 5.0],\n",
       " ['Sepal.Length', 4.4],\n",
       " ['Sepal.Length', 4.9],\n",
       " ['Sepal.Length', 5.4],\n",
       " ['Sepal.Length', 4.8],\n",
       " ['Sepal.Length', 4.8],\n",
       " ['Sepal.Length', 4.3],\n",
       " ['Sepal.Length', 5.8],\n",
       " ['Sepal.Length', 5.7],\n",
       " ['Sepal.Length', 5.4],\n",
       " ['Sepal.Length', 5.1],\n",
       " ['Sepal.Length', 5.7],\n",
       " ['Sepal.Length', 5.1],\n",
       " ['Sepal.Length', 5.4],\n",
       " ['Sepal.Length', 5.1],\n",
       " ['Sepal.Length', 4.6],\n",
       " ['Sepal.Length', 5.1],\n",
       " ['Sepal.Length', 4.8],\n",
       " ['Sepal.Length', 5.0],\n",
       " ['Sepal.Length', 5.0],\n",
       " ['Sepal.Length', 5.2],\n",
       " ['Sepal.Length', 5.2],\n",
       " ['Sepal.Length', 4.7],\n",
       " ['Sepal.Length', 4.8],\n",
       " ['Sepal.Length', 5.4],\n",
       " ['Sepal.Length', 5.2],\n",
       " ['Sepal.Length', 5.5],\n",
       " ['Sepal.Length', 4.9],\n",
       " ['Sepal.Length', 5.0],\n",
       " ['Sepal.Length', 5.5],\n",
       " ['Sepal.Length', 4.9],\n",
       " ['Sepal.Length', 4.4],\n",
       " ['Sepal.Length', 5.1],\n",
       " ['Sepal.Length', 5.0],\n",
       " ['Sepal.Length', 4.5],\n",
       " ['Sepal.Length', 4.4],\n",
       " ['Sepal.Length', 5.0],\n",
       " ['Sepal.Length', 5.1],\n",
       " ['Sepal.Length', 4.8],\n",
       " ['Sepal.Length', 5.1],\n",
       " ['Sepal.Length', 4.6],\n",
       " ['Sepal.Length', 5.3],\n",
       " ['Sepal.Length', 5.0],\n",
       " ['Sepal.Length', 7.0],\n",
       " ['Sepal.Length', 6.4],\n",
       " ['Sepal.Length', 6.9],\n",
       " ['Sepal.Length', 5.5],\n",
       " ['Sepal.Length', 6.5],\n",
       " ['Sepal.Length', 5.7],\n",
       " ['Sepal.Length', 6.3],\n",
       " ['Sepal.Length', 4.9],\n",
       " ['Sepal.Length', 6.6],\n",
       " ['Sepal.Length', 5.2],\n",
       " ['Sepal.Length', 5.0],\n",
       " ['Sepal.Length', 5.9],\n",
       " ['Sepal.Length', 6.0],\n",
       " ['Sepal.Length', 6.1],\n",
       " ['Sepal.Length', 5.6],\n",
       " ['Sepal.Length', 6.7],\n",
       " ['Sepal.Length', 5.6],\n",
       " ['Sepal.Length', 5.8],\n",
       " ['Sepal.Length', 6.2],\n",
       " ['Sepal.Length', 5.6],\n",
       " ['Sepal.Length', 5.9],\n",
       " ['Sepal.Length', 6.1],\n",
       " ['Sepal.Length', 6.3],\n",
       " ['Sepal.Length', 6.1],\n",
       " ['Sepal.Length', 6.4],\n",
       " ['Sepal.Length', 6.6],\n",
       " ['Sepal.Length', 6.8],\n",
       " ['Sepal.Length', 6.7],\n",
       " ['Sepal.Length', 6.0],\n",
       " ['Sepal.Length', 5.7],\n",
       " ['Sepal.Length', 5.5],\n",
       " ['Sepal.Length', 5.5],\n",
       " ['Sepal.Length', 5.8],\n",
       " ['Sepal.Length', 6.0],\n",
       " ['Sepal.Length', 5.4],\n",
       " ['Sepal.Length', 6.0],\n",
       " ['Sepal.Length', 6.7],\n",
       " ['Sepal.Length', 6.3],\n",
       " ['Sepal.Length', 5.6],\n",
       " ['Sepal.Length', 5.5],\n",
       " ['Sepal.Length', 5.5],\n",
       " ['Sepal.Length', 6.1],\n",
       " ['Sepal.Length', 5.8],\n",
       " ['Sepal.Length', 5.0],\n",
       " ['Sepal.Length', 5.6],\n",
       " ['Sepal.Length', 5.7],\n",
       " ['Sepal.Length', 5.7],\n",
       " ['Sepal.Length', 6.2],\n",
       " ['Sepal.Length', 5.1],\n",
       " ['Sepal.Length', 5.7],\n",
       " ['Sepal.Length', 6.3],\n",
       " ['Sepal.Length', 5.8],\n",
       " ['Sepal.Length', 7.1],\n",
       " ['Sepal.Length', 6.3],\n",
       " ['Sepal.Length', 6.5],\n",
       " ['Sepal.Length', 7.6],\n",
       " ['Sepal.Length', 4.9],\n",
       " ['Sepal.Length', 7.3],\n",
       " ['Sepal.Length', 6.7],\n",
       " ['Sepal.Length', 7.2],\n",
       " ['Sepal.Length', 6.5],\n",
       " ['Sepal.Length', 6.4],\n",
       " ['Sepal.Length', 6.8],\n",
       " ['Sepal.Length', 5.7],\n",
       " ['Sepal.Length', 5.8],\n",
       " ['Sepal.Length', 6.4],\n",
       " ['Sepal.Length', 6.5],\n",
       " ['Sepal.Length', 7.7],\n",
       " ['Sepal.Length', 7.7],\n",
       " ['Sepal.Length', 6.0],\n",
       " ['Sepal.Length', 6.9],\n",
       " ['Sepal.Length', 5.6],\n",
       " ['Sepal.Length', 7.7],\n",
       " ['Sepal.Length', 6.3],\n",
       " ['Sepal.Length', 6.7],\n",
       " ['Sepal.Length', 7.2],\n",
       " ['Sepal.Length', 6.2],\n",
       " ['Sepal.Length', 6.1],\n",
       " ['Sepal.Length', 6.4],\n",
       " ['Sepal.Length', 7.2],\n",
       " ['Sepal.Length', 7.4],\n",
       " ['Sepal.Length', 7.9],\n",
       " ['Sepal.Length', 6.4],\n",
       " ['Sepal.Length', 6.3],\n",
       " ['Sepal.Length', 6.1],\n",
       " ['Sepal.Length', 7.7],\n",
       " ['Sepal.Length', 6.3],\n",
       " ['Sepal.Length', 6.4],\n",
       " ['Sepal.Length', 6.0],\n",
       " ['Sepal.Length', 6.9],\n",
       " ['Sepal.Length', 6.7],\n",
       " ['Sepal.Length', 6.9],\n",
       " ['Sepal.Length', 5.8],\n",
       " ['Sepal.Length', 6.8],\n",
       " ['Sepal.Length', 6.7],\n",
       " ['Sepal.Length', 6.7],\n",
       " ['Sepal.Length', 6.3],\n",
       " ['Sepal.Length', 6.5],\n",
       " ['Sepal.Length', 6.2],\n",
       " ['Sepal.Length', 5.9],\n",
       " ['Sepal.Width', 3.5],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 3.2],\n",
       " ['Sepal.Width', 3.1],\n",
       " ['Sepal.Width', 3.6],\n",
       " ['Sepal.Width', 3.9],\n",
       " ['Sepal.Width', 3.4],\n",
       " ['Sepal.Width', 3.4],\n",
       " ['Sepal.Width', 2.9],\n",
       " ['Sepal.Width', 3.1],\n",
       " ['Sepal.Width', 3.7],\n",
       " ['Sepal.Width', 3.4],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 4.0],\n",
       " ['Sepal.Width', 4.4],\n",
       " ['Sepal.Width', 3.9],\n",
       " ['Sepal.Width', 3.5],\n",
       " ['Sepal.Width', 3.8],\n",
       " ['Sepal.Width', 3.8],\n",
       " ['Sepal.Width', 3.4],\n",
       " ['Sepal.Width', 3.7],\n",
       " ['Sepal.Width', 3.6],\n",
       " ['Sepal.Width', 3.3],\n",
       " ['Sepal.Width', 3.4],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 3.4],\n",
       " ['Sepal.Width', 3.5],\n",
       " ['Sepal.Width', 3.4],\n",
       " ['Sepal.Width', 3.2],\n",
       " ['Sepal.Width', 3.1],\n",
       " ['Sepal.Width', 3.4],\n",
       " ['Sepal.Width', 4.1],\n",
       " ['Sepal.Width', 4.2],\n",
       " ['Sepal.Width', 3.1],\n",
       " ['Sepal.Width', 3.2],\n",
       " ['Sepal.Width', 3.5],\n",
       " ['Sepal.Width', 3.6],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 3.4],\n",
       " ['Sepal.Width', 3.5],\n",
       " ['Sepal.Width', 2.3],\n",
       " ['Sepal.Width', 3.2],\n",
       " ['Sepal.Width', 3.5],\n",
       " ['Sepal.Width', 3.8],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 3.8],\n",
       " ['Sepal.Width', 3.2],\n",
       " ['Sepal.Width', 3.7],\n",
       " ['Sepal.Width', 3.3],\n",
       " ['Sepal.Width', 3.2],\n",
       " ['Sepal.Width', 3.2],\n",
       " ['Sepal.Width', 3.1],\n",
       " ['Sepal.Width', 2.3],\n",
       " ['Sepal.Width', 2.8],\n",
       " ['Sepal.Width', 2.8],\n",
       " ['Sepal.Width', 3.3],\n",
       " ['Sepal.Width', 2.4],\n",
       " ['Sepal.Width', 2.9],\n",
       " ['Sepal.Width', 2.7],\n",
       " ['Sepal.Width', 2.0],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 2.2],\n",
       " ['Sepal.Width', 2.9],\n",
       " ['Sepal.Width', 2.9],\n",
       " ['Sepal.Width', 3.1],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 2.7],\n",
       " ['Sepal.Width', 2.2],\n",
       " ['Sepal.Width', 2.5],\n",
       " ['Sepal.Width', 3.2],\n",
       " ['Sepal.Width', 2.8],\n",
       " ['Sepal.Width', 2.5],\n",
       " ['Sepal.Width', 2.8],\n",
       " ['Sepal.Width', 2.9],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 2.8],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 2.9],\n",
       " ['Sepal.Width', 2.6],\n",
       " ['Sepal.Width', 2.4],\n",
       " ['Sepal.Width', 2.4],\n",
       " ['Sepal.Width', 2.7],\n",
       " ['Sepal.Width', 2.7],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 3.4],\n",
       " ['Sepal.Width', 3.1],\n",
       " ['Sepal.Width', 2.3],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 2.5],\n",
       " ['Sepal.Width', 2.6],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 2.6],\n",
       " ['Sepal.Width', 2.3],\n",
       " ['Sepal.Width', 2.7],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 2.9],\n",
       " ['Sepal.Width', 2.9],\n",
       " ['Sepal.Width', 2.5],\n",
       " ['Sepal.Width', 2.8],\n",
       " ['Sepal.Width', 3.3],\n",
       " ['Sepal.Width', 2.7],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 2.9],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 2.5],\n",
       " ['Sepal.Width', 2.9],\n",
       " ['Sepal.Width', 2.5],\n",
       " ['Sepal.Width', 3.6],\n",
       " ['Sepal.Width', 3.2],\n",
       " ['Sepal.Width', 2.7],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 2.5],\n",
       " ['Sepal.Width', 2.8],\n",
       " ['Sepal.Width', 3.2],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 3.8],\n",
       " ['Sepal.Width', 2.6],\n",
       " ['Sepal.Width', 2.2],\n",
       " ['Sepal.Width', 3.2],\n",
       " ['Sepal.Width', 2.8],\n",
       " ['Sepal.Width', 2.8],\n",
       " ['Sepal.Width', 2.7],\n",
       " ['Sepal.Width', 3.3],\n",
       " ['Sepal.Width', 3.2],\n",
       " ['Sepal.Width', 2.8],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 2.8],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 2.8],\n",
       " ['Sepal.Width', 3.8],\n",
       " ['Sepal.Width', 2.8],\n",
       " ['Sepal.Width', 2.8],\n",
       " ['Sepal.Width', 2.6],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 3.4],\n",
       " ['Sepal.Width', 3.1],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 3.1],\n",
       " ['Sepal.Width', 3.1],\n",
       " ['Sepal.Width', 3.1],\n",
       " ['Sepal.Width', 2.7],\n",
       " ['Sepal.Width', 3.2],\n",
       " ['Sepal.Width', 3.3],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 2.5],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Sepal.Width', 3.4],\n",
       " ['Sepal.Width', 3.0],\n",
       " ['Petal.Length', 1.4],\n",
       " ['Petal.Length', 1.4],\n",
       " ['Petal.Length', 1.3],\n",
       " ['Petal.Length', 1.5],\n",
       " ['Petal.Length', 1.4],\n",
       " ['Petal.Length', 1.7],\n",
       " ['Petal.Length', 1.4],\n",
       " ['Petal.Length', 1.5],\n",
       " ['Petal.Length', 1.4],\n",
       " ['Petal.Length', 1.5],\n",
       " ['Petal.Length', 1.5],\n",
       " ['Petal.Length', 1.6],\n",
       " ['Petal.Length', 1.4],\n",
       " ['Petal.Length', 1.1],\n",
       " ['Petal.Length', 1.2],\n",
       " ['Petal.Length', 1.5],\n",
       " ['Petal.Length', 1.3],\n",
       " ['Petal.Length', 1.4],\n",
       " ['Petal.Length', 1.7],\n",
       " ['Petal.Length', 1.5],\n",
       " ['Petal.Length', 1.7],\n",
       " ['Petal.Length', 1.5],\n",
       " ['Petal.Length', 1.0],\n",
       " ['Petal.Length', 1.7],\n",
       " ['Petal.Length', 1.9],\n",
       " ['Petal.Length', 1.6],\n",
       " ['Petal.Length', 1.6],\n",
       " ['Petal.Length', 1.5],\n",
       " ['Petal.Length', 1.4],\n",
       " ['Petal.Length', 1.6],\n",
       " ['Petal.Length', 1.6],\n",
       " ['Petal.Length', 1.5],\n",
       " ['Petal.Length', 1.5],\n",
       " ['Petal.Length', 1.4],\n",
       " ['Petal.Length', 1.5],\n",
       " ['Petal.Length', 1.2],\n",
       " ['Petal.Length', 1.3],\n",
       " ['Petal.Length', 1.4],\n",
       " ['Petal.Length', 1.3],\n",
       " ['Petal.Length', 1.5],\n",
       " ['Petal.Length', 1.3],\n",
       " ['Petal.Length', 1.3],\n",
       " ['Petal.Length', 1.3],\n",
       " ['Petal.Length', 1.6],\n",
       " ['Petal.Length', 1.9],\n",
       " ['Petal.Length', 1.4],\n",
       " ['Petal.Length', 1.6],\n",
       " ['Petal.Length', 1.4],\n",
       " ['Petal.Length', 1.5],\n",
       " ['Petal.Length', 1.4],\n",
       " ['Petal.Length', 4.7],\n",
       " ['Petal.Length', 4.5],\n",
       " ['Petal.Length', 4.9],\n",
       " ['Petal.Length', 4.0],\n",
       " ['Petal.Length', 4.6],\n",
       " ['Petal.Length', 4.5],\n",
       " ['Petal.Length', 4.7],\n",
       " ['Petal.Length', 3.3],\n",
       " ['Petal.Length', 4.6],\n",
       " ['Petal.Length', 3.9],\n",
       " ['Petal.Length', 3.5],\n",
       " ['Petal.Length', 4.2],\n",
       " ['Petal.Length', 4.0],\n",
       " ['Petal.Length', 4.7],\n",
       " ['Petal.Length', 3.6],\n",
       " ['Petal.Length', 4.4],\n",
       " ['Petal.Length', 4.5],\n",
       " ['Petal.Length', 4.1],\n",
       " ['Petal.Length', 4.5],\n",
       " ['Petal.Length', 3.9],\n",
       " ['Petal.Length', 4.8],\n",
       " ['Petal.Length', 4.0],\n",
       " ['Petal.Length', 4.9],\n",
       " ['Petal.Length', 4.7],\n",
       " ['Petal.Length', 4.3],\n",
       " ['Petal.Length', 4.4],\n",
       " ['Petal.Length', 4.8],\n",
       " ['Petal.Length', 5.0],\n",
       " ['Petal.Length', 4.5],\n",
       " ['Petal.Length', 3.5],\n",
       " ['Petal.Length', 3.8],\n",
       " ['Petal.Length', 3.7],\n",
       " ['Petal.Length', 3.9],\n",
       " ['Petal.Length', 5.1],\n",
       " ['Petal.Length', 4.5],\n",
       " ['Petal.Length', 4.5],\n",
       " ['Petal.Length', 4.7],\n",
       " ['Petal.Length', 4.4],\n",
       " ['Petal.Length', 4.1],\n",
       " ['Petal.Length', 4.0],\n",
       " ['Petal.Length', 4.4],\n",
       " ['Petal.Length', 4.6],\n",
       " ['Petal.Length', 4.0],\n",
       " ['Petal.Length', 3.3],\n",
       " ['Petal.Length', 4.2],\n",
       " ['Petal.Length', 4.2],\n",
       " ['Petal.Length', 4.2],\n",
       " ['Petal.Length', 4.3],\n",
       " ['Petal.Length', 3.0],\n",
       " ['Petal.Length', 4.1],\n",
       " ['Petal.Length', 6.0],\n",
       " ['Petal.Length', 5.1],\n",
       " ['Petal.Length', 5.9],\n",
       " ['Petal.Length', 5.6],\n",
       " ['Petal.Length', 5.8],\n",
       " ['Petal.Length', 6.6],\n",
       " ['Petal.Length', 4.5],\n",
       " ['Petal.Length', 6.3],\n",
       " ['Petal.Length', 5.8],\n",
       " ['Petal.Length', 6.1],\n",
       " ['Petal.Length', 5.1],\n",
       " ['Petal.Length', 5.3],\n",
       " ['Petal.Length', 5.5],\n",
       " ['Petal.Length', 5.0],\n",
       " ['Petal.Length', 5.1],\n",
       " ['Petal.Length', 5.3],\n",
       " ['Petal.Length', 5.5],\n",
       " ['Petal.Length', 6.7],\n",
       " ['Petal.Length', 6.9],\n",
       " ['Petal.Length', 5.0],\n",
       " ['Petal.Length', 5.7],\n",
       " ['Petal.Length', 4.9],\n",
       " ['Petal.Length', 6.7],\n",
       " ['Petal.Length', 4.9],\n",
       " ['Petal.Length', 5.7],\n",
       " ['Petal.Length', 6.0],\n",
       " ['Petal.Length', 4.8],\n",
       " ['Petal.Length', 4.9],\n",
       " ['Petal.Length', 5.6],\n",
       " ['Petal.Length', 5.8],\n",
       " ['Petal.Length', 6.1],\n",
       " ['Petal.Length', 6.4],\n",
       " ['Petal.Length', 5.6],\n",
       " ['Petal.Length', 5.1],\n",
       " ['Petal.Length', 5.6],\n",
       " ['Petal.Length', 6.1],\n",
       " ['Petal.Length', 5.6],\n",
       " ['Petal.Length', 5.5],\n",
       " ['Petal.Length', 4.8],\n",
       " ['Petal.Length', 5.4],\n",
       " ['Petal.Length', 5.6],\n",
       " ['Petal.Length', 5.1],\n",
       " ['Petal.Length', 5.1],\n",
       " ['Petal.Length', 5.9],\n",
       " ['Petal.Length', 5.7],\n",
       " ['Petal.Length', 5.2],\n",
       " ['Petal.Length', 5.0],\n",
       " ['Petal.Length', 5.2],\n",
       " ['Petal.Length', 5.4],\n",
       " ['Petal.Length', 5.1],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.4],\n",
       " ['Petal.Width', 0.3],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.1],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.1],\n",
       " ['Petal.Width', 0.1],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.4],\n",
       " ['Petal.Width', 0.4],\n",
       " ['Petal.Width', 0.3],\n",
       " ['Petal.Width', 0.3],\n",
       " ['Petal.Width', 0.3],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.4],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.5],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.4],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.4],\n",
       " ['Petal.Width', 0.1],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.1],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.3],\n",
       " ['Petal.Width', 0.3],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.6],\n",
       " ['Petal.Width', 0.4],\n",
       " ['Petal.Width', 0.3],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 0.2],\n",
       " ['Petal.Width', 1.4],\n",
       " ['Petal.Width', 1.5],\n",
       " ['Petal.Width', 1.5],\n",
       " ['Petal.Width', 1.3],\n",
       " ['Petal.Width', 1.5],\n",
       " ['Petal.Width', 1.3],\n",
       " ['Petal.Width', 1.6],\n",
       " ['Petal.Width', 1.0],\n",
       " ['Petal.Width', 1.3],\n",
       " ['Petal.Width', 1.4],\n",
       " ['Petal.Width', 1.0],\n",
       " ['Petal.Width', 1.5],\n",
       " ['Petal.Width', 1.0],\n",
       " ['Petal.Width', 1.4],\n",
       " ['Petal.Width', 1.3],\n",
       " ['Petal.Width', 1.4],\n",
       " ['Petal.Width', 1.5],\n",
       " ['Petal.Width', 1.0],\n",
       " ['Petal.Width', 1.5],\n",
       " ['Petal.Width', 1.1],\n",
       " ['Petal.Width', 1.8],\n",
       " ['Petal.Width', 1.3],\n",
       " ['Petal.Width', 1.5],\n",
       " ['Petal.Width', 1.2],\n",
       " ['Petal.Width', 1.3],\n",
       " ['Petal.Width', 1.4],\n",
       " ['Petal.Width', 1.4],\n",
       " ['Petal.Width', 1.7],\n",
       " ['Petal.Width', 1.5],\n",
       " ['Petal.Width', 1.0],\n",
       " ['Petal.Width', 1.1],\n",
       " ['Petal.Width', 1.0],\n",
       " ['Petal.Width', 1.2],\n",
       " ['Petal.Width', 1.6],\n",
       " ['Petal.Width', 1.5],\n",
       " ['Petal.Width', 1.6],\n",
       " ['Petal.Width', 1.5],\n",
       " ['Petal.Width', 1.3],\n",
       " ['Petal.Width', 1.3],\n",
       " ['Petal.Width', 1.3],\n",
       " ['Petal.Width', 1.2],\n",
       " ['Petal.Width', 1.4],\n",
       " ['Petal.Width', 1.2],\n",
       " ['Petal.Width', 1.0],\n",
       " ['Petal.Width', 1.3],\n",
       " ['Petal.Width', 1.2],\n",
       " ['Petal.Width', 1.3],\n",
       " ['Petal.Width', 1.3],\n",
       " ['Petal.Width', 1.1],\n",
       " ['Petal.Width', 1.3],\n",
       " ['Petal.Width', 2.5],\n",
       " ['Petal.Width', 1.9],\n",
       " ['Petal.Width', 2.1],\n",
       " ['Petal.Width', 1.8],\n",
       " ['Petal.Width', 2.2],\n",
       " ['Petal.Width', 2.1],\n",
       " ['Petal.Width', 1.7],\n",
       " ['Petal.Width', 1.8],\n",
       " ['Petal.Width', 1.8],\n",
       " ['Petal.Width', 2.5],\n",
       " ['Petal.Width', 2.0],\n",
       " ['Petal.Width', 1.9],\n",
       " ['Petal.Width', 2.1],\n",
       " ['Petal.Width', 2.0],\n",
       " ['Petal.Width', 2.4],\n",
       " ['Petal.Width', 2.3],\n",
       " ['Petal.Width', 1.8],\n",
       " ['Petal.Width', 2.2],\n",
       " ['Petal.Width', 2.3],\n",
       " ['Petal.Width', 1.5],\n",
       " ['Petal.Width', 2.3],\n",
       " ['Petal.Width', 2.0],\n",
       " ['Petal.Width', 2.0],\n",
       " ['Petal.Width', 1.8],\n",
       " ['Petal.Width', 2.1],\n",
       " ['Petal.Width', 1.8],\n",
       " ['Petal.Width', 1.8],\n",
       " ['Petal.Width', 1.8],\n",
       " ['Petal.Width', 2.1],\n",
       " ['Petal.Width', 1.6],\n",
       " ['Petal.Width', 1.9],\n",
       " ['Petal.Width', 2.0],\n",
       " ['Petal.Width', 2.2],\n",
       " ['Petal.Width', 1.5],\n",
       " ['Petal.Width', 1.4],\n",
       " ['Petal.Width', 2.3],\n",
       " ['Petal.Width', 2.4],\n",
       " ['Petal.Width', 1.8],\n",
       " ['Petal.Width', 1.8],\n",
       " ['Petal.Width', 2.1],\n",
       " ['Petal.Width', 2.4],\n",
       " ['Petal.Width', 2.3],\n",
       " ['Petal.Width', 1.9],\n",
       " ['Petal.Width', 2.3],\n",
       " ['Petal.Width', 2.5],\n",
       " ['Petal.Width', 2.3],\n",
       " ['Petal.Width', 1.9],\n",
       " ['Petal.Width', 2.0],\n",
       " ['Petal.Width', 2.3],\n",
       " ['Petal.Width', 1.8]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "There are situations where data of a single data set is split among multiple RDDs.\n",
    "Certain computation requires all the data be present in a single location / RDD.\n",
    "Contents of multiple RDDs can be combined with the help of union command.\n",
    "'''\n",
    "\n",
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "SL = iris1_split.map(lambda var1: ['Sepal.Length',float(var1[0])])\n",
    "SW = iris1_split.map(lambda var1: ['Sepal.Width',float(var1[1])])\n",
    "PL = iris1_split.map(lambda var1: ['Petal.Length',float(var1[2])])\n",
    "PW = iris1_split.map(lambda var1: ['Petal.Width',float(var1[3])])\n",
    "CV = iris1_split.map(lambda var1: ['Species',var1[4]])\n",
    "print(SL.take(10))\n",
    "print(\"*\"*50)\n",
    "print(SW.take(10))\n",
    "print(\"*\"*50)\n",
    "print(PL.take(10))\n",
    "print(\"*\"*50)\n",
    "print(PW.take(10))\n",
    "print(\"*\"*50)\n",
    "union_data = spark.sparkContext.union([SL,SW,PL,PW])\n",
    "union_data.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'd']\n"
     ]
    }
   ],
   "source": [
    "# Intersection\n",
    "EmployeeList = spark.sparkContext.parallelize(['a','b','c','d','e'])\n",
    "HealthMemberShip = spark.sparkContext.parallelize(['d','e','f','g'])\n",
    "print(EmployeeList.intersection(HealthMemberShip).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('employee2', ([2, 4, 1, 2, 2], [2, 1, 1, 1, 2])), ('employee1', ([3, 1, 2, 5, 4], [4, 5, 2, 1, 1]))]\n"
     ]
    }
   ],
   "source": [
    "# Two RDDs containing data in the form of the key-value pair can be joined using 'join' function\n",
    "\n",
    "location1 = spark.sparkContext.parallelize([('employee1',[3,1,2,5,4]),('employee2',[2,4,1,2,2])])\n",
    "location2 = spark.sparkContext.parallelize([('employee2',[2,1,1,1,2]),('employee1',[4,5,2,1,1])])\n",
    "join1 = location1.join(location2)\n",
    "print(join1.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['employee2', [2, 1, 1, 1, 2]], ['employee1', [4, 5, 2, 1, 1]]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can extract the column values of the second RDD, along with its key as shown below.\n",
    "join1.map(lambda var1: [var1[0],var1[1][1]]).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('283', ['363', '20080101', '20080113', '20080108', '594', '283', '1', '2', 3.0, 1376.994, 0.0, 0.0, 1251.9813, 3755.9439, 4130.982, 103.2746]), ('283', ['483', '20080301', '20080313', '20080308', '90', '283', '1', '5', 4.0, 72.0, 0.0, 0.0, 44.88, 179.52, 288.0, 7.2]), ('283', ['381', '20070201', '20070213', '20070208', '666', '283', '1', '2', 1.0, 600.2625, 0.0, 0.0, 605.6492, 605.6492, 600.2625, 15.0066]), ('283', ['454', '20060901', '20060913', '20060908', '130', '283', '1', '4', 4.0, 35.994, 0.0, 0.0, 24.7459, 98.9836, 143.976, 3.5994])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#import tables as RDD\n",
    "DimEmployee = spark.sparkContext.textFile(\"dataset/AdventureWorks/AdventureWorks_RDD/DimEmployee.csv\")\n",
    "FactResellerSales = spark.sparkContext.textFile(\"dataset/AdventureWorks/AdventureWorks_RDD/FactResellerSales.csv\")\n",
    "\n",
    "#split data based on delimiter\n",
    "DimEmployee = DimEmployee.map(lambda var1: var1.split(\",\"))\n",
    "FactResellerSales = FactResellerSales.map(lambda var1: var1.split(\",\"))\n",
    "\n",
    "#convert data in-terms of key-value pairs where the key is the column to be joined\n",
    "DimEmployee = DimEmployee.map(lambda var1: [var1[0],[var1[1],var1[2],var1[3]]])\n",
    "FactResellerSales = FactResellerSales.map(lambda var1: [var1[5], [var1[0], var1[1], var1[2], var1[3], var1[4], var1[5], var1[6], var1[7], float(var1[8]), float(var1[9]),float(var1[10]),float(var1[11]), float(var1[12]), float(var1[13]), float(var1[14]), float(var1[15])]])\n",
    "\n",
    "#perform join\n",
    "RF = FactResellerSales.join(DimEmployee)\n",
    "\n",
    "#print contents of joins\n",
    "RF.take(10)\n",
    "\n",
    "#extract first RDD values along with the key\n",
    "print(RF.map(lambda var1: (var1[0],var1[1][0])).take(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop Through Values - foreach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "iris1_mod = iris1_split.flatMap(lambda var1:(('Sepal.Length',float(var1[0])),('Sepal.Width',float(var1[1])),('Petal.Length',float(var1[2])),('Petal.Width',float(var1[3]))))\n",
    "\n",
    "def fun(x): print(x)\n",
    "iris1_mod.foreach(fun)\n",
    "# output is printed on console\n",
    "#In this example, foreach loop has been used to iterate through all \n",
    "# the key-value pairs of 'iris1_mod' RDD printing its contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Data Based on a Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['7.1', '3.0', '5.9', '2.1', 'virginica'],\n",
       " ['7.6', '3.0', '6.6', '2.1', 'virginica'],\n",
       " ['7.3', '2.9', '6.3', '1.8', 'virginica'],\n",
       " ['7.2', '3.6', '6.1', '2.5', 'virginica'],\n",
       " ['7.7', '3.8', '6.7', '2.2', 'virginica'],\n",
       " ['7.7', '2.6', '6.9', '2.3', 'virginica'],\n",
       " ['7.7', '2.8', '6.7', '2.0', 'virginica'],\n",
       " ['7.2', '3.2', '6.0', '1.8', 'virginica'],\n",
       " ['7.2', '3.0', '5.8', '1.6', 'virginica'],\n",
       " ['7.4', '2.8', '6.1', '1.9', 'virginica'],\n",
       " ['7.9', '3.8', '6.4', '2.0', 'virginica'],\n",
       " ['7.7', '3.0', '6.1', '2.3', 'virginica']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Our objective is to remove all the records whose 'Sepal_Length' \n",
    "(first column) value is less than 7. To achieve the same, \n",
    "we can make use of filter function which is used to filter data based on a condition. \n",
    "'''\n",
    "\n",
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "filter1 = iris1_split.filter(lambda x: float(x[0]) > 7)\n",
    "filter1.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.0, 6.9, 6.6, 6.7, 6.6, 6.8, 6.7, 6.7, 7.1, 7.6]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1_split.map(lambda var1: float(var1[0])).filter(lambda var1 : var1>6.5).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.0, 6.9, 6.6, 6.7, 6.6, 6.8, 6.7, 6.7, 7.1, 7.6]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1_split.filter(lambda var1 : float(var1[0])>6.5).map(lambda var1: float(var1[0])).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Descriptive Statistics using RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Multiple RDD Data With Same Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('b', 4)]\n",
      "[('a', 2)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x1e9a41d53c8>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x1e9a41d5f60>)),\n",
       " ('b',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x1e9a41d5ac8>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x1e9a41d5da0>))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Consider a scenario where there are key-value pairs in two separate RDDs,\n",
    "and we are required to collate all the data with the same key together, \n",
    "from both the RDDs. To achieve the same, we can make use of 'cogroup' function.\n",
    "'''\n",
    "p1 = spark.sparkContext.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "p2 = spark.sparkContext.parallelize([(\"a\", 2)])\n",
    "print(p1.collect())\n",
    "print(p2.collect())\n",
    "p1.cogroup(p2).collect()\n",
    "\n",
    "# for each key, there is a collection of values as a result of 'cogroup' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', ([1], [2])), ('b', ([4], []))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The result of cogroup function can be realized by combining for loop with tuple,\n",
    "map and list function. It has already been seen that for each key, \n",
    "we get an iterable object. Using for loop we iterate through each key, \n",
    "value pair of the cogroup result, and we place each 'value' in a list, \n",
    "and the collection of the list is in turn placed inside a tuple.\n",
    "'''\n",
    "\n",
    "p1 = spark.sparkContext.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "p2 = spark.sparkContext.parallelize([(\"a\", 2)])\n",
    "[(x, tuple(map(list, y))) for x, y in p1.cogroup(p2).collect()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('non-setosa', ([6], [4], [], [7]))\n",
      "('setosa', ([5], [1], [2], []))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Consider a scenario where there are key-value pairs in multiple RDDs, \n",
    "and we are required to collate all the data with the same key together, \n",
    "from all the RDDs. To achieve the same, we can make use of 'groupWith' function.\n",
    "\n",
    "In short, groupWith works exactly like cogroup, except that it can deal with more than RDDs.\n",
    "'''\n",
    "\n",
    "data1 = spark.sparkContext.parallelize([(\"setosa\", 5), (\"non-setosa\", 6)])\n",
    "data2 = spark.sparkContext.parallelize([(\"setosa\", 1), (\"non-setosa\", 4)])\n",
    "data3 = spark.sparkContext.parallelize([(\"setosa\", 2)])\n",
    "data4 = spark.sparkContext.parallelize([(\"non-setosa\", 7)])\n",
    "groupwith1 = data1.groupWith(data2, data3, data4).collect()\n",
    "for x,y in groupwith1:\n",
    "    print((x, tuple(map(list, y))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('setosa', 2), ('non-setosa', 1)])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Now, the objective is to find out the total occurrence of each species of iris dataset.\n",
    "We can achieve the same by using the countByKey function. \n",
    "Let us first try to understand the same using test data. In the below example,\n",
    "a test data with three key-value pairs are created. The value can be anything, \n",
    "but the key should be the element to be counted.\n",
    "'''\n",
    "p1 = spark.sparkContext.parallelize([(\"setosa\", 1), (\"non-setosa\", 1), (\"setosa\", 1)])\n",
    "print(p1.countByKey().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('setosa', 50), ('versicolor', 50), ('virginica', 50)])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The total occurrence of values in a particular column can be calculated \n",
    "using the countByValue function. \n",
    "It finds out the distinct occurrence of each value in that particular column.\n",
    "'''\n",
    "\n",
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "iris1_mod = iris1_split.map(lambda var1:(var1[4],var1))\n",
    "\n",
    "print(iris1_mod.countByKey().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('setosa', 50), ('versicolor', 50), ('virginica', 50)])\n"
     ]
    }
   ],
   "source": [
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "print(iris1_split.map(lambda col: col[4]).countByValue().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "# Counting Total Number of Values\n",
    "# The total number of values in a particular data set can be calculated using the count function.\n",
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "print(iris1.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa', 'versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "iris1_mod = iris1_split.map(lambda col:col[4])\n",
    "distinct1 = iris1_mod.distinct()\n",
    "print(distinct1.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Sequence of Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "range1 = spark.sparkContext.range(start=1,end=10,step=2)\n",
    "print(range1.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Function for Each Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sepal_Length', 15), ('Sepal_Width', 12)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If it required to apply some function to values of a particular key,\n",
    "# we can make use of the mapValues function.\n",
    "\n",
    "data1 = spark.sparkContext.parallelize([(\"Sepal_Length\", [2,1,3,5,4]), (\"Sepal_Width\", [3,1,2,4,2])])\n",
    "\n",
    "def fun(para1): \n",
    "    return sum(para1)\n",
    "\n",
    "data1.mapValues(fun).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "458.6000000000001"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure data in an RDD can be aggregated using 'fold' function.\n",
    "# In this example, values of 'Sepal.Width' has been aggregated.\n",
    "\n",
    "from operator import add\n",
    "\n",
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "iris1_mod = iris1_split.map(lambda col:col[1])\n",
    "\n",
    "iris1_split.map(lambda col:float(col[1])).fold(0,add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Petal.Length', 563.7000000000002),\n",
       " ('Petal.Width', 179.89999999999998),\n",
       " ('Sepal.Length', 876.4999999999998),\n",
       " ('Sepal.Width', 458.6000000000001)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Aggregate by key\n",
    "=================\n",
    "Assume a scenario where data is present as a key-value pair, where column name\n",
    "is the key and its corresponding value for a particular record is its value. So,\n",
    "if there are 150 rows with 4 columns, there would be 150*4 key-value pairs. \n",
    "It is required to aggregate values in each column. In such scenario, we can make\n",
    "use of 'foldByKey' to aggregate the values for each key. Since, the key is the \n",
    "name of the column, all the values within a particular column will get aggregated.\n",
    "'''\n",
    "\n",
    "from operator import add\n",
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "iris1_mod = iris1_split.flatMap(lambda var1:(('Sepal.Length',float(var1[0])),('Sepal.Width',float(var1[1])),('Petal.Length',float(var1[2])),('Petal.Width',float(var1[3]))))\n",
    "\n",
    "iris1_mod.foldByKey(0, add).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "876.4999999999998\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Reduce\n",
    "======\n",
    "Reduce function is used to reduce the elements of an RDD (usually used for aggregation).\n",
    "It takes an argument, which is the function to be executed on the RDD data.\n",
    "It is to be noted that the function that is passed as an argument should be\n",
    "commutative and associative in nature.\n",
    "\n",
    "Reduce function is an 'Action' of RDD. So, it would execute all operations previously\n",
    "recorded in the lineage. Then it would execute the function passed as an argument.\n",
    "'''\n",
    "\n",
    "from operator import add\n",
    "\n",
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "iris1_mod = iris1_split.map(lambda var1:float(var1[0]))\n",
    "print(iris1_mod.reduce(add))\n",
    "# we have made use of reduce to aggregate the values of 'Sepal_Length' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Petal.Length', 563.7000000000002), ('Petal.Width', 179.89999999999998), ('Sepal.Length', 876.4999999999998), ('Sepal.Width', 458.6000000000001)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "reduceByKey\n",
    "===========\n",
    "Similar to reduce, there is another function called 'reduceByKey' which works\n",
    "exactly as that of reduce function, except that the function passed as an argument\n",
    "to 'reduceByKey' function is applied once for all the values in particular key. \n",
    "It can be used in situations where it is required to aggregate values for each group of data.\n",
    "'''\n",
    "\n",
    "from operator import add\n",
    "\n",
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "iris1_mod = iris1_split.flatMap(lambda var1:(('Sepal.Length',float(var1[0])),('Sepal.Width',float(var1[1])),('Petal.Length',float(var1[2])),('Petal.Width',float(var1[3]))))\n",
    "print(iris1_mod.reduceByKey(add).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Petal.Length', <pyspark.resultiterable.ResultIterable object at 0x000001E9A4194588>), ('Petal.Width', <pyspark.resultiterable.ResultIterable object at 0x000001E9A4194C50>), ('Sepal.Length', <pyspark.resultiterable.ResultIterable object at 0x000001E9A41946A0>), ('Sepal.Width', <pyspark.resultiterable.ResultIterable object at 0x000001E9A4194940>)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "groupByKey\n",
    "==========\n",
    "We can group elements with the same key using 'groupByKey' function. In this example,\n",
    "key is the name of the column. On applying groupByKey function we group data for each column.\n",
    "'''\n",
    "\n",
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "iris1_mod = iris1_split.flatMap(lambda var1:(('Sepal.Length',float(var1[0])),('Sepal.Width',float(var1[1])),('Petal.Length',float(var1[2])),('Petal.Width',float(var1[3]))))\n",
    "print(iris1_mod.groupByKey().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Petal.Length', [1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.6, 1.4, 1.1, 1.2, 1.5, 1.3, 1.4, 1.7, 1.5, 1.7, 1.5, 1.0, 1.7, 1.9, 1.6, 1.6, 1.5, 1.4, 1.6, 1.6, 1.5, 1.5, 1.4, 1.5, 1.2, 1.3, 1.4, 1.3, 1.5, 1.3, 1.3, 1.3, 1.6, 1.9, 1.4, 1.6, 1.4, 1.5, 1.4, 4.7, 4.5, 4.9, 4.0, 4.6, 4.5, 4.7, 3.3, 4.6, 3.9, 3.5, 4.2, 4.0, 4.7, 3.6, 4.4, 4.5, 4.1, 4.5, 3.9, 4.8, 4.0, 4.9, 4.7, 4.3, 4.4, 4.8, 5.0, 4.5, 3.5, 3.8, 3.7, 3.9, 5.1, 4.5, 4.5, 4.7, 4.4, 4.1, 4.0, 4.4, 4.6, 4.0, 3.3, 4.2, 4.2, 4.2, 4.3, 3.0, 4.1, 6.0, 5.1, 5.9, 5.6, 5.8, 6.6, 4.5, 6.3, 5.8, 6.1, 5.1, 5.3, 5.5, 5.0, 5.1, 5.3, 5.5, 6.7, 6.9, 5.0, 5.7, 4.9, 6.7, 4.9, 5.7, 6.0, 4.8, 4.9, 5.6, 5.8, 6.1, 6.4, 5.6, 5.1, 5.6, 6.1, 5.6, 5.5, 4.8, 5.4, 5.6, 5.1, 5.1, 5.9, 5.7, 5.2, 5.0, 5.2, 5.4, 5.1]), ('Petal.Width', [0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.2, 0.1, 0.1, 0.2, 0.4, 0.4, 0.3, 0.3, 0.3, 0.2, 0.4, 0.2, 0.5, 0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.4, 0.1, 0.2, 0.2, 0.2, 0.2, 0.1, 0.2, 0.2, 0.3, 0.3, 0.2, 0.6, 0.4, 0.3, 0.2, 0.2, 0.2, 0.2, 1.4, 1.5, 1.5, 1.3, 1.5, 1.3, 1.6, 1.0, 1.3, 1.4, 1.0, 1.5, 1.0, 1.4, 1.3, 1.4, 1.5, 1.0, 1.5, 1.1, 1.8, 1.3, 1.5, 1.2, 1.3, 1.4, 1.4, 1.7, 1.5, 1.0, 1.1, 1.0, 1.2, 1.6, 1.5, 1.6, 1.5, 1.3, 1.3, 1.3, 1.2, 1.4, 1.2, 1.0, 1.3, 1.2, 1.3, 1.3, 1.1, 1.3, 2.5, 1.9, 2.1, 1.8, 2.2, 2.1, 1.7, 1.8, 1.8, 2.5, 2.0, 1.9, 2.1, 2.0, 2.4, 2.3, 1.8, 2.2, 2.3, 1.5, 2.3, 2.0, 2.0, 1.8, 2.1, 1.8, 1.8, 1.8, 2.1, 1.6, 1.9, 2.0, 2.2, 1.5, 1.4, 2.3, 2.4, 1.8, 1.8, 2.1, 2.4, 2.3, 1.9, 2.3, 2.5, 2.3, 1.9, 2.0, 2.3, 1.8])]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "It was seen that the grouped result of 'groupByKey' function was an iterable\n",
    "object.To convert the same to list or any other object form, we can make use\n",
    "of mapValues along with groupByKey as shown below.\n",
    "'''\n",
    "\n",
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "iris1_mod = iris1_split.flatMap(lambda var1:(('Sepal.Length',float(var1[0])),('Sepal.Width',float(var1[1])),('Petal.Length', float(var1[2])), ('Petal.Width', float(var1[3]))))\n",
    "group1 = iris1_mod.groupByKey()\n",
    "print(group1.mapValues(list).take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Petal.Length', 563.7000000000004), ('Petal.Width', 179.90000000000012), ('Sepal.Length', 876.5000000000002), ('Sepal.Width', 458.60000000000014)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Aggregating Grouped RDD\n",
    "Aggregation can be performed on grouped data by combining the aggregate function\n",
    "with mapValues and groupByKey function\n",
    "'''\n",
    "\n",
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "iris1_mod = iris1_split.flatMap(lambda var1:(('Sepal.Length',float(var1[0])),('Sepal.Width',float(var1[1])),('Petal.Length',float(var1[2])),('Petal.Width',float(var1[3]))))\n",
    "print(iris1_mod.groupByKey().mapValues(sum).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Minimum, Maximum and Mean of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min:  4.3\n",
      "max:  7.9\n",
      "mean:  5.843333333333332\n"
     ]
    }
   ],
   "source": [
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "iris1_mod = iris1_split.map(lambda var1:float(var1[0]))\n",
    "print(\"min: \", iris1_mod.min())\n",
    "print(\"max: \", iris1_mod.max())\n",
    "print(\"mean: \", iris1_mod.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring Spread of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stddev:  0.8253012917851412\n",
      "variance:  0.6811222222222227\n",
      "**************************************************\n",
      "(count: 150, mean: 5.843333333333332, stdev: 0.8253012917851412, max: 7.9, min: 4.3)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "One of the methods to understand the spread of the data is by calculating standard deviation or variance.\n",
    "Standard Deviation of data in an RDD can be calculated using the 'stdev' function.\n",
    "'''\n",
    "\n",
    "iris1 = spark.sparkContext.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "iris1_mod = iris1_split.map(lambda var1:float(var1[0]))\n",
    "print(\"stddev: \", iris1_mod.stdev())\n",
    "print(\"variance: \", iris1_mod.variance())\n",
    "\n",
    "# Statistical summary of an RDD can be viewed in a single shot using the stats function.\n",
    "print(\"*\"*50)\n",
    "print(iris1_mod.stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Sepal_Length='5.1', Sepal_Width='3.5', Petal_Length='1.4', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='4.9', Sepal_Width='3.0', Petal_Length='1.4', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='4.7', Sepal_Width='3.2', Petal_Length='1.3', Petal_Width='0.2', Species='setosa')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read CSV is similar to Pandas\n",
    "\n",
    "iris_df = spark.read.csv(r\"dataset\\iris\\iris.csv\", header=True)\n",
    "iris_df.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
