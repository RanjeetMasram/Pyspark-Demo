{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intellesence in Jupyter Notebook\n",
    "%config IPCompleter.greedy=True\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Operation on DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entry point to programming Spark with the Data Frame is Spark session.\n",
    "With a SQL Context, applications can create Data Frames from an existing RDD, from a Hive table, or from data sources.\n",
    "\n",
    "When working in interactive mode the Spark Context, Spark Session, SQL Context are by default created with names 'sc',' spark', 'sqlContext' respectively.\n",
    "\n",
    "In case of PySpark script Spark Context, Spark Session, SQL Context needs to be manually created. An object of Spark Context is required for the creation of SQL Context object and Spark Session object. Below is shown an example where Spark Session and SQL Context is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pyspark library\n",
    "import findspark\n",
    "\n",
    "# specifying the path of SPARK_HOME\n",
    "findspark.init(r\"D:\\Study\\PySpark\\Pysparksetup\\spark\")\n",
    "\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()\n",
    "# importing SparkSession to create Spark session\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "#creating Spark Context - already discussed\n",
    "sc = SparkContext(master='local',appName='test1')\n",
    "#creating Spark Session\n",
    "spark = SparkSession(sc)\n",
    "#creating SQL Context\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing data using Spark Session\n",
    "df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "#importing data using SQL Context\n",
    "df2 = sqlContext.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Sepal_Length: string, Sepal_Width: string, Petal_Length: string, Petal_Width: string, Species: string]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The data frame object 'df1' and 'df2' doesn't contain the actual data, \n",
    "but instead, contains the lineage, similar to an RDD object.\n",
    "On printing the data frame object it shows the metadata about the imported data frame object.\n",
    "'''\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Petal_Length: double, Petal_Width: double, Sepal_Length: double, Sepal_Width: double, Species: string]\n"
     ]
    }
   ],
   "source": [
    "iris1_df1 = spark.read.json('dataset/iris/iris.json')\n",
    "print(iris1_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert RDD to Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+------+\n",
      "| _1| _2| _3| _4|    _5|\n",
      "+---+---+---+---+------+\n",
      "|5.1|3.5|1.4|0.2|setosa|\n",
      "|4.9|3.0|1.4|0.2|setosa|\n",
      "|4.7|3.2|1.3|0.2|setosa|\n",
      "|4.6|3.1|1.5|0.2|setosa|\n",
      "|5.0|3.6|1.4|0.2|setosa|\n",
      "|5.4|3.9|1.7|0.4|setosa|\n",
      "|4.6|3.4|1.4|0.3|setosa|\n",
      "|5.0|3.4|1.5|0.2|setosa|\n",
      "|4.4|2.9|1.4|0.2|setosa|\n",
      "|4.9|3.1|1.5|0.1|setosa|\n",
      "+---+---+---+---+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris1 = sc.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda line: line.split(\",\"))\n",
    "df1=spark.createDataFrame(iris1_split)\n",
    "df1.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Data Frame to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5.1', '3.5', '1.4', '0.2', 'setosa'),\n",
       " ('4.9', '3.0', '1.4', '0.2', 'setosa'),\n",
       " ('4.7', '3.2', '1.3', '0.2', 'setosa'),\n",
       " ('4.6', '3.1', '1.5', '0.2', 'setosa'),\n",
       " ('5.0', '3.6', '1.4', '0.2', 'setosa'),\n",
       " ('5.4', '3.9', '1.7', '0.4', 'setosa'),\n",
       " ('4.6', '3.4', '1.4', '0.3', 'setosa'),\n",
       " ('5.0', '3.4', '1.5', '0.2', 'setosa'),\n",
       " ('4.4', '2.9', '1.4', '0.2', 'setosa'),\n",
       " ('4.9', '3.1', '1.5', '0.1', 'setosa')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "iris1_df1.rdd.map(tuple).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Contents of Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|Sepal_Length|Sepal_Width|Petal_Length|Petal_Width|Species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Display Contents of Data Frame in Table Format\n",
    "==============================================\n",
    "Contents of a data frame can be viewed using the show function. \n",
    "It takes an argument 'n' which is the total number of lines to be displayed.\n",
    "'''\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "iris1_df1.show(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Sepal_Length='5.1', Sepal_Width='3.5', Petal_Length='1.4', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='4.9', Sepal_Width='3.0', Petal_Length='1.4', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='4.7', Sepal_Width='3.2', Petal_Length='1.3', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='4.6', Sepal_Width='3.1', Petal_Length='1.5', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='5.0', Sepal_Width='3.6', Petal_Length='1.4', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='5.4', Sepal_Width='3.9', Petal_Length='1.7', Petal_Width='0.4', Species='setosa'),\n",
       " Row(Sepal_Length='4.6', Sepal_Width='3.4', Petal_Length='1.4', Petal_Width='0.3', Species='setosa'),\n",
       " Row(Sepal_Length='5.0', Sepal_Width='3.4', Petal_Length='1.5', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='4.4', Sepal_Width='2.9', Petal_Length='1.4', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='4.9', Sepal_Width='3.1', Petal_Length='1.5', Petal_Width='0.1', Species='setosa'),\n",
       " Row(Sepal_Length='5.4', Sepal_Width='3.7', Petal_Length='1.5', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='4.8', Sepal_Width='3.4', Petal_Length='1.6', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='4.8', Sepal_Width='3.0', Petal_Length='1.4', Petal_Width='0.1', Species='setosa'),\n",
       " Row(Sepal_Length='4.3', Sepal_Width='3.0', Petal_Length='1.1', Petal_Width='0.1', Species='setosa'),\n",
       " Row(Sepal_Length='5.8', Sepal_Width='4.0', Petal_Length='1.2', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='5.7', Sepal_Width='4.4', Petal_Length='1.5', Petal_Width='0.4', Species='setosa'),\n",
       " Row(Sepal_Length='5.4', Sepal_Width='3.9', Petal_Length='1.3', Petal_Width='0.4', Species='setosa'),\n",
       " Row(Sepal_Length='5.1', Sepal_Width='3.5', Petal_Length='1.4', Petal_Width='0.3', Species='setosa'),\n",
       " Row(Sepal_Length='5.7', Sepal_Width='3.8', Petal_Length='1.7', Petal_Width='0.3', Species='setosa'),\n",
       " Row(Sepal_Length='5.1', Sepal_Width='3.8', Petal_Length='1.5', Petal_Width='0.3', Species='setosa'),\n",
       " Row(Sepal_Length='5.4', Sepal_Width='3.4', Petal_Length='1.7', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='5.1', Sepal_Width='3.7', Petal_Length='1.5', Petal_Width='0.4', Species='setosa'),\n",
       " Row(Sepal_Length='4.6', Sepal_Width='3.6', Petal_Length='1.0', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='5.1', Sepal_Width='3.3', Petal_Length='1.7', Petal_Width='0.5', Species='setosa'),\n",
       " Row(Sepal_Length='4.8', Sepal_Width='3.4', Petal_Length='1.9', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='5.0', Sepal_Width='3.0', Petal_Length='1.6', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='5.0', Sepal_Width='3.4', Petal_Length='1.6', Petal_Width='0.4', Species='setosa'),\n",
       " Row(Sepal_Length='5.2', Sepal_Width='3.5', Petal_Length='1.5', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='5.2', Sepal_Width='3.4', Petal_Length='1.4', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='4.7', Sepal_Width='3.2', Petal_Length='1.6', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='4.8', Sepal_Width='3.1', Petal_Length='1.6', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='5.4', Sepal_Width='3.4', Petal_Length='1.5', Petal_Width='0.4', Species='setosa'),\n",
       " Row(Sepal_Length='5.2', Sepal_Width='4.1', Petal_Length='1.5', Petal_Width='0.1', Species='setosa'),\n",
       " Row(Sepal_Length='5.5', Sepal_Width='4.2', Petal_Length='1.4', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='4.9', Sepal_Width='3.1', Petal_Length='1.5', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='5.0', Sepal_Width='3.2', Petal_Length='1.2', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='5.5', Sepal_Width='3.5', Petal_Length='1.3', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='4.9', Sepal_Width='3.6', Petal_Length='1.4', Petal_Width='0.1', Species='setosa'),\n",
       " Row(Sepal_Length='4.4', Sepal_Width='3.0', Petal_Length='1.3', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='5.1', Sepal_Width='3.4', Petal_Length='1.5', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='5.0', Sepal_Width='3.5', Petal_Length='1.3', Petal_Width='0.3', Species='setosa'),\n",
       " Row(Sepal_Length='4.5', Sepal_Width='2.3', Petal_Length='1.3', Petal_Width='0.3', Species='setosa'),\n",
       " Row(Sepal_Length='4.4', Sepal_Width='3.2', Petal_Length='1.3', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='5.0', Sepal_Width='3.5', Petal_Length='1.6', Petal_Width='0.6', Species='setosa'),\n",
       " Row(Sepal_Length='5.1', Sepal_Width='3.8', Petal_Length='1.9', Petal_Width='0.4', Species='setosa'),\n",
       " Row(Sepal_Length='4.8', Sepal_Width='3.0', Petal_Length='1.4', Petal_Width='0.3', Species='setosa'),\n",
       " Row(Sepal_Length='5.1', Sepal_Width='3.8', Petal_Length='1.6', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='4.6', Sepal_Width='3.2', Petal_Length='1.4', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='5.3', Sepal_Width='3.7', Petal_Length='1.5', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='5.0', Sepal_Width='3.3', Petal_Length='1.4', Petal_Width='0.2', Species='setosa'),\n",
       " Row(Sepal_Length='7.0', Sepal_Width='3.2', Petal_Length='4.7', Petal_Width='1.4', Species='versicolor'),\n",
       " Row(Sepal_Length='6.4', Sepal_Width='3.2', Petal_Length='4.5', Petal_Width='1.5', Species='versicolor'),\n",
       " Row(Sepal_Length='6.9', Sepal_Width='3.1', Petal_Length='4.9', Petal_Width='1.5', Species='versicolor'),\n",
       " Row(Sepal_Length='5.5', Sepal_Width='2.3', Petal_Length='4.0', Petal_Width='1.3', Species='versicolor'),\n",
       " Row(Sepal_Length='6.5', Sepal_Width='2.8', Petal_Length='4.6', Petal_Width='1.5', Species='versicolor'),\n",
       " Row(Sepal_Length='5.7', Sepal_Width='2.8', Petal_Length='4.5', Petal_Width='1.3', Species='versicolor'),\n",
       " Row(Sepal_Length='6.3', Sepal_Width='3.3', Petal_Length='4.7', Petal_Width='1.6', Species='versicolor'),\n",
       " Row(Sepal_Length='4.9', Sepal_Width='2.4', Petal_Length='3.3', Petal_Width='1.0', Species='versicolor'),\n",
       " Row(Sepal_Length='6.6', Sepal_Width='2.9', Petal_Length='4.6', Petal_Width='1.3', Species='versicolor'),\n",
       " Row(Sepal_Length='5.2', Sepal_Width='2.7', Petal_Length='3.9', Petal_Width='1.4', Species='versicolor'),\n",
       " Row(Sepal_Length='5.0', Sepal_Width='2.0', Petal_Length='3.5', Petal_Width='1.0', Species='versicolor'),\n",
       " Row(Sepal_Length='5.9', Sepal_Width='3.0', Petal_Length='4.2', Petal_Width='1.5', Species='versicolor'),\n",
       " Row(Sepal_Length='6.0', Sepal_Width='2.2', Petal_Length='4.0', Petal_Width='1.0', Species='versicolor'),\n",
       " Row(Sepal_Length='6.1', Sepal_Width='2.9', Petal_Length='4.7', Petal_Width='1.4', Species='versicolor'),\n",
       " Row(Sepal_Length='5.6', Sepal_Width='2.9', Petal_Length='3.6', Petal_Width='1.3', Species='versicolor'),\n",
       " Row(Sepal_Length='6.7', Sepal_Width='3.1', Petal_Length='4.4', Petal_Width='1.4', Species='versicolor'),\n",
       " Row(Sepal_Length='5.6', Sepal_Width='3.0', Petal_Length='4.5', Petal_Width='1.5', Species='versicolor'),\n",
       " Row(Sepal_Length='5.8', Sepal_Width='2.7', Petal_Length='4.1', Petal_Width='1.0', Species='versicolor'),\n",
       " Row(Sepal_Length='6.2', Sepal_Width='2.2', Petal_Length='4.5', Petal_Width='1.5', Species='versicolor'),\n",
       " Row(Sepal_Length='5.6', Sepal_Width='2.5', Petal_Length='3.9', Petal_Width='1.1', Species='versicolor'),\n",
       " Row(Sepal_Length='5.9', Sepal_Width='3.2', Petal_Length='4.8', Petal_Width='1.8', Species='versicolor'),\n",
       " Row(Sepal_Length='6.1', Sepal_Width='2.8', Petal_Length='4.0', Petal_Width='1.3', Species='versicolor'),\n",
       " Row(Sepal_Length='6.3', Sepal_Width='2.5', Petal_Length='4.9', Petal_Width='1.5', Species='versicolor'),\n",
       " Row(Sepal_Length='6.1', Sepal_Width='2.8', Petal_Length='4.7', Petal_Width='1.2', Species='versicolor'),\n",
       " Row(Sepal_Length='6.4', Sepal_Width='2.9', Petal_Length='4.3', Petal_Width='1.3', Species='versicolor'),\n",
       " Row(Sepal_Length='6.6', Sepal_Width='3.0', Petal_Length='4.4', Petal_Width='1.4', Species='versicolor'),\n",
       " Row(Sepal_Length='6.8', Sepal_Width='2.8', Petal_Length='4.8', Petal_Width='1.4', Species='versicolor'),\n",
       " Row(Sepal_Length='6.7', Sepal_Width='3.0', Petal_Length='5.0', Petal_Width='1.7', Species='versicolor'),\n",
       " Row(Sepal_Length='6.0', Sepal_Width='2.9', Petal_Length='4.5', Petal_Width='1.5', Species='versicolor'),\n",
       " Row(Sepal_Length='5.7', Sepal_Width='2.6', Petal_Length='3.5', Petal_Width='1.0', Species='versicolor'),\n",
       " Row(Sepal_Length='5.5', Sepal_Width='2.4', Petal_Length='3.8', Petal_Width='1.1', Species='versicolor'),\n",
       " Row(Sepal_Length='5.5', Sepal_Width='2.4', Petal_Length='3.7', Petal_Width='1.0', Species='versicolor'),\n",
       " Row(Sepal_Length='5.8', Sepal_Width='2.7', Petal_Length='3.9', Petal_Width='1.2', Species='versicolor'),\n",
       " Row(Sepal_Length='6.0', Sepal_Width='2.7', Petal_Length='5.1', Petal_Width='1.6', Species='versicolor'),\n",
       " Row(Sepal_Length='5.4', Sepal_Width='3.0', Petal_Length='4.5', Petal_Width='1.5', Species='versicolor'),\n",
       " Row(Sepal_Length='6.0', Sepal_Width='3.4', Petal_Length='4.5', Petal_Width='1.6', Species='versicolor'),\n",
       " Row(Sepal_Length='6.7', Sepal_Width='3.1', Petal_Length='4.7', Petal_Width='1.5', Species='versicolor'),\n",
       " Row(Sepal_Length='6.3', Sepal_Width='2.3', Petal_Length='4.4', Petal_Width='1.3', Species='versicolor'),\n",
       " Row(Sepal_Length='5.6', Sepal_Width='3.0', Petal_Length='4.1', Petal_Width='1.3', Species='versicolor'),\n",
       " Row(Sepal_Length='5.5', Sepal_Width='2.5', Petal_Length='4.0', Petal_Width='1.3', Species='versicolor'),\n",
       " Row(Sepal_Length='5.5', Sepal_Width='2.6', Petal_Length='4.4', Petal_Width='1.2', Species='versicolor'),\n",
       " Row(Sepal_Length='6.1', Sepal_Width='3.0', Petal_Length='4.6', Petal_Width='1.4', Species='versicolor'),\n",
       " Row(Sepal_Length='5.8', Sepal_Width='2.6', Petal_Length='4.0', Petal_Width='1.2', Species='versicolor'),\n",
       " Row(Sepal_Length='5.0', Sepal_Width='2.3', Petal_Length='3.3', Petal_Width='1.0', Species='versicolor'),\n",
       " Row(Sepal_Length='5.6', Sepal_Width='2.7', Petal_Length='4.2', Petal_Width='1.3', Species='versicolor'),\n",
       " Row(Sepal_Length='5.7', Sepal_Width='3.0', Petal_Length='4.2', Petal_Width='1.2', Species='versicolor'),\n",
       " Row(Sepal_Length='5.7', Sepal_Width='2.9', Petal_Length='4.2', Petal_Width='1.3', Species='versicolor'),\n",
       " Row(Sepal_Length='6.2', Sepal_Width='2.9', Petal_Length='4.3', Petal_Width='1.3', Species='versicolor'),\n",
       " Row(Sepal_Length='5.1', Sepal_Width='2.5', Petal_Length='3.0', Petal_Width='1.1', Species='versicolor'),\n",
       " Row(Sepal_Length='5.7', Sepal_Width='2.8', Petal_Length='4.1', Petal_Width='1.3', Species='versicolor'),\n",
       " Row(Sepal_Length='6.3', Sepal_Width='3.3', Petal_Length='6.0', Petal_Width='2.5', Species='virginica'),\n",
       " Row(Sepal_Length='5.8', Sepal_Width='2.7', Petal_Length='5.1', Petal_Width='1.9', Species='virginica'),\n",
       " Row(Sepal_Length='7.1', Sepal_Width='3.0', Petal_Length='5.9', Petal_Width='2.1', Species='virginica'),\n",
       " Row(Sepal_Length='6.3', Sepal_Width='2.9', Petal_Length='5.6', Petal_Width='1.8', Species='virginica'),\n",
       " Row(Sepal_Length='6.5', Sepal_Width='3.0', Petal_Length='5.8', Petal_Width='2.2', Species='virginica'),\n",
       " Row(Sepal_Length='7.6', Sepal_Width='3.0', Petal_Length='6.6', Petal_Width='2.1', Species='virginica'),\n",
       " Row(Sepal_Length='4.9', Sepal_Width='2.5', Petal_Length='4.5', Petal_Width='1.7', Species='virginica'),\n",
       " Row(Sepal_Length='7.3', Sepal_Width='2.9', Petal_Length='6.3', Petal_Width='1.8', Species='virginica'),\n",
       " Row(Sepal_Length='6.7', Sepal_Width='2.5', Petal_Length='5.8', Petal_Width='1.8', Species='virginica'),\n",
       " Row(Sepal_Length='7.2', Sepal_Width='3.6', Petal_Length='6.1', Petal_Width='2.5', Species='virginica'),\n",
       " Row(Sepal_Length='6.5', Sepal_Width='3.2', Petal_Length='5.1', Petal_Width='2.0', Species='virginica'),\n",
       " Row(Sepal_Length='6.4', Sepal_Width='2.7', Petal_Length='5.3', Petal_Width='1.9', Species='virginica'),\n",
       " Row(Sepal_Length='6.8', Sepal_Width='3.0', Petal_Length='5.5', Petal_Width='2.1', Species='virginica'),\n",
       " Row(Sepal_Length='5.7', Sepal_Width='2.5', Petal_Length='5.0', Petal_Width='2.0', Species='virginica'),\n",
       " Row(Sepal_Length='5.8', Sepal_Width='2.8', Petal_Length='5.1', Petal_Width='2.4', Species='virginica'),\n",
       " Row(Sepal_Length='6.4', Sepal_Width='3.2', Petal_Length='5.3', Petal_Width='2.3', Species='virginica'),\n",
       " Row(Sepal_Length='6.5', Sepal_Width='3.0', Petal_Length='5.5', Petal_Width='1.8', Species='virginica'),\n",
       " Row(Sepal_Length='7.7', Sepal_Width='3.8', Petal_Length='6.7', Petal_Width='2.2', Species='virginica'),\n",
       " Row(Sepal_Length='7.7', Sepal_Width='2.6', Petal_Length='6.9', Petal_Width='2.3', Species='virginica'),\n",
       " Row(Sepal_Length='6.0', Sepal_Width='2.2', Petal_Length='5.0', Petal_Width='1.5', Species='virginica'),\n",
       " Row(Sepal_Length='6.9', Sepal_Width='3.2', Petal_Length='5.7', Petal_Width='2.3', Species='virginica'),\n",
       " Row(Sepal_Length='5.6', Sepal_Width='2.8', Petal_Length='4.9', Petal_Width='2.0', Species='virginica'),\n",
       " Row(Sepal_Length='7.7', Sepal_Width='2.8', Petal_Length='6.7', Petal_Width='2.0', Species='virginica'),\n",
       " Row(Sepal_Length='6.3', Sepal_Width='2.7', Petal_Length='4.9', Petal_Width='1.8', Species='virginica'),\n",
       " Row(Sepal_Length='6.7', Sepal_Width='3.3', Petal_Length='5.7', Petal_Width='2.1', Species='virginica'),\n",
       " Row(Sepal_Length='7.2', Sepal_Width='3.2', Petal_Length='6.0', Petal_Width='1.8', Species='virginica'),\n",
       " Row(Sepal_Length='6.2', Sepal_Width='2.8', Petal_Length='4.8', Petal_Width='1.8', Species='virginica'),\n",
       " Row(Sepal_Length='6.1', Sepal_Width='3.0', Petal_Length='4.9', Petal_Width='1.8', Species='virginica'),\n",
       " Row(Sepal_Length='6.4', Sepal_Width='2.8', Petal_Length='5.6', Petal_Width='2.1', Species='virginica'),\n",
       " Row(Sepal_Length='7.2', Sepal_Width='3.0', Petal_Length='5.8', Petal_Width='1.6', Species='virginica'),\n",
       " Row(Sepal_Length='7.4', Sepal_Width='2.8', Petal_Length='6.1', Petal_Width='1.9', Species='virginica'),\n",
       " Row(Sepal_Length='7.9', Sepal_Width='3.8', Petal_Length='6.4', Petal_Width='2.0', Species='virginica'),\n",
       " Row(Sepal_Length='6.4', Sepal_Width='2.8', Petal_Length='5.6', Petal_Width='2.2', Species='virginica'),\n",
       " Row(Sepal_Length='6.3', Sepal_Width='2.8', Petal_Length='5.1', Petal_Width='1.5', Species='virginica'),\n",
       " Row(Sepal_Length='6.1', Sepal_Width='2.6', Petal_Length='5.6', Petal_Width='1.4', Species='virginica'),\n",
       " Row(Sepal_Length='7.7', Sepal_Width='3.0', Petal_Length='6.1', Petal_Width='2.3', Species='virginica'),\n",
       " Row(Sepal_Length='6.3', Sepal_Width='3.4', Petal_Length='5.6', Petal_Width='2.4', Species='virginica'),\n",
       " Row(Sepal_Length='6.4', Sepal_Width='3.1', Petal_Length='5.5', Petal_Width='1.8', Species='virginica'),\n",
       " Row(Sepal_Length='6.0', Sepal_Width='3.0', Petal_Length='4.8', Petal_Width='1.8', Species='virginica'),\n",
       " Row(Sepal_Length='6.9', Sepal_Width='3.1', Petal_Length='5.4', Petal_Width='2.1', Species='virginica'),\n",
       " Row(Sepal_Length='6.7', Sepal_Width='3.1', Petal_Length='5.6', Petal_Width='2.4', Species='virginica'),\n",
       " Row(Sepal_Length='6.9', Sepal_Width='3.1', Petal_Length='5.1', Petal_Width='2.3', Species='virginica'),\n",
       " Row(Sepal_Length='5.8', Sepal_Width='2.7', Petal_Length='5.1', Petal_Width='1.9', Species='virginica'),\n",
       " Row(Sepal_Length='6.8', Sepal_Width='3.2', Petal_Length='5.9', Petal_Width='2.3', Species='virginica'),\n",
       " Row(Sepal_Length='6.7', Sepal_Width='3.3', Petal_Length='5.7', Petal_Width='2.5', Species='virginica'),\n",
       " Row(Sepal_Length='6.7', Sepal_Width='3.0', Petal_Length='5.2', Petal_Width='2.3', Species='virginica'),\n",
       " Row(Sepal_Length='6.3', Sepal_Width='2.5', Petal_Length='5.0', Petal_Width='1.9', Species='virginica'),\n",
       " Row(Sepal_Length='6.5', Sepal_Width='3.0', Petal_Length='5.2', Petal_Width='2.0', Species='virginica'),\n",
       " Row(Sepal_Length='6.2', Sepal_Width='3.4', Petal_Length='5.4', Petal_Width='2.3', Species='virginica'),\n",
       " Row(Sepal_Length='5.9', Sepal_Width='3.0', Petal_Length='5.1', Petal_Width='1.8', Species='virginica')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Display Contents of Data Frame as a List of Rows\n",
    "================================================\n",
    "All the contents of a data frame can be sent back to the driver program as a\n",
    "list of rows objects using the collect function.\n",
    "'''\n",
    "\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "iris1_df1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Displays First 'n' Rows of Data Frame as a List of Rows\n",
    "=======================================================\n",
    "First 'n' rows of a data frame can be sent back to the driver program \n",
    "as a list of rows objects using head function.\n",
    "'''\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "iris1_df1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|Sepal_Length|Species|\n",
      "+------------+-------+\n",
      "|         5.1| setosa|\n",
      "|         4.9| setosa|\n",
      "|         4.7| setosa|\n",
      "|         4.6| setosa|\n",
      "|         5.0| setosa|\n",
      "|         5.4| setosa|\n",
      "|         4.6| setosa|\n",
      "|         5.0| setosa|\n",
      "|         4.4| setosa|\n",
      "|         4.9| setosa|\n",
      "|         5.4| setosa|\n",
      "|         4.8| setosa|\n",
      "|         4.8| setosa|\n",
      "|         4.3| setosa|\n",
      "|         5.8| setosa|\n",
      "|         5.7| setosa|\n",
      "|         5.4| setosa|\n",
      "|         5.1| setosa|\n",
      "|         5.7| setosa|\n",
      "|         5.1| setosa|\n",
      "+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Data Selection\n",
    "===============\n",
    "Any particular column of a data frame can be selected by specifying\n",
    "the name of the column in the 'select' function.\n",
    "'''\n",
    "\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "iris1_df1.select(\"Sepal_Length\",\"Species\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+-------+\n",
      "| ID|Sepal_Length|Sepal_Width|Petal_Length|Petal_Width|Species|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "|  1|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|  2|         4.9|          3|         1.4|        0.2| setosa|\n",
      "|  3|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|  4|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|  5|           5|        3.6|         1.4|        0.2| setosa|\n",
      "|  6|         5.4|        3.9|         1.7|        0.4| setosa|\n",
      "|  7|         4.6|        3.4|         1.4|        0.3| setosa|\n",
      "|  8|           5|        3.4|         1.5|        0.2| setosa|\n",
      "|  9|         4.4|        2.9|         1.4|        0.2| setosa|\n",
      "| 10|         4.9|        3.1|         1.5|        0.1| setosa|\n",
      "| 11|         5.4|        3.7|         1.5|        0.2| setosa|\n",
      "| 12|         4.8|        3.4|         1.6|        0.2| setosa|\n",
      "| 13|         4.8|          3|         1.4|        0.1| setosa|\n",
      "| 14|         4.3|          3|         1.1|        0.1| setosa|\n",
      "| 15|         5.8|          4|         1.2|        0.2| setosa|\n",
      "| 16|         5.7|        4.4|         1.5|        0.4| setosa|\n",
      "| 17|         5.4|        3.9|         1.3|        0.4| setosa|\n",
      "| 18|         5.1|        3.5|         1.4|        0.3| setosa|\n",
      "| 19|         5.7|        3.8|         1.7|        0.3| setosa|\n",
      "| 20|         5.1|        3.8|         1.5|        0.3| setosa|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris1_df1 = spark.read.csv(path='dataset/iris/merge/iris_merge1.csv',sep=',',header=True)\n",
    "iris1_df2 = spark.read.csv(path='dataset/iris/merge/iris_merge2.csv',sep=',',header=True)\n",
    "iris1_df1.join(other=iris1_df2,on='ID',how='inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|Sepal_Length|Petal_Length|\n",
      "+------------+------------+\n",
      "|         5.1|         1.4|\n",
      "|         4.9|         1.4|\n",
      "|         4.7|         1.3|\n",
      "|         4.6|         1.5|\n",
      "|           5|         1.4|\n",
      "|         5.4|         1.7|\n",
      "|         4.6|         1.4|\n",
      "|           5|         1.5|\n",
      "|         4.4|         1.4|\n",
      "|         4.9|         1.5|\n",
      "|         5.4|         1.5|\n",
      "|         4.8|         1.6|\n",
      "|         4.8|         1.4|\n",
      "|         4.3|         1.1|\n",
      "|         5.8|         1.2|\n",
      "|         5.7|         1.5|\n",
      "|         5.4|         1.3|\n",
      "|         5.1|         1.4|\n",
      "|         5.7|         1.7|\n",
      "|         5.1|         1.5|\n",
      "+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Once two data frames are joined, required columns from the two table\n",
    "#can be retrieved using select function along with the join function.\n",
    "\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/merge/iris_merge1.csv',sep=',',header=True)\n",
    "iris1_df2 = spark.read.csv(path='dataset/iris/merge/iris_merge2.csv',sep=',',header=True)\n",
    "iris1_df1.join(other=iris1_df2,on='ID',how='inner').select(iris1_df1.Sepal_Length,iris1_df2.Petal_Length).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+---+---+------------+-----------+-------+\n",
      "|Sepal_Length|Sepal_Width| ID| ID|Petal_Length|Petal_Width|Species|\n",
      "+------------+-----------+---+---+------------+-----------+-------+\n",
      "|         5.1|        3.5|  1|  1|         1.4|        0.2| setosa|\n",
      "|         4.9|          3|  2|  2|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|  3|  3|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|  4|  4|         1.5|        0.2| setosa|\n",
      "|           5|        3.6|  5|  5|         1.4|        0.2| setosa|\n",
      "|         5.4|        3.9|  6|  6|         1.7|        0.4| setosa|\n",
      "|         4.6|        3.4|  7|  7|         1.4|        0.3| setosa|\n",
      "|           5|        3.4|  8|  8|         1.5|        0.2| setosa|\n",
      "|         4.4|        2.9|  9|  9|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.1| 10| 10|         1.5|        0.1| setosa|\n",
      "|         5.4|        3.7| 11| 11|         1.5|        0.2| setosa|\n",
      "|         4.8|        3.4| 12| 12|         1.6|        0.2| setosa|\n",
      "|         4.8|          3| 13| 13|         1.4|        0.1| setosa|\n",
      "|         4.3|          3| 14| 14|         1.1|        0.1| setosa|\n",
      "|         5.8|          4| 15| 15|         1.2|        0.2| setosa|\n",
      "|         5.7|        4.4| 16| 16|         1.5|        0.4| setosa|\n",
      "|         5.4|        3.9| 17| 17|         1.3|        0.4| setosa|\n",
      "|         5.1|        3.5| 18| 18|         1.4|        0.3| setosa|\n",
      "|         5.7|        3.8| 19| 19|         1.7|        0.3| setosa|\n",
      "|         5.1|        3.8| 20| 20|         1.5|        0.3| setosa|\n",
      "+------------+-----------+---+---+------------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Below is shown another example of how to join two tables where the joining columns present\n",
    "# in the two tables have a different name.\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/merge/iris_merge1.csv',sep=',',header=True)\n",
    "iris1_df2 = spark.read.csv(path='dataset/iris/merge/iris_merge2.csv',sep=',',header=True) \n",
    "iris1_df1.join(other=iris1_df2,on=(iris1_df1.ID==iris1_df2.ID),how='inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+\n",
      "|Sepal.Length|Sepal.Width|Petal.Length|Petal.Width|\n",
      "+------------+-----------+------------+-----------+\n",
      "|           5|          3|           1|          0|\n",
      "|         4.6|       null|           2|        0.1|\n",
      "|         7.2|        3.1|         5.1|          1|\n",
      "|           8|          4|           7|          2|\n",
      "|          10|          6|           2|          0|\n",
      "|         9.2|          0|           4|        0.2|\n",
      "|        14.4|        6.2|        10.2|          2|\n",
      "|          16|          8|          14|          4|\n",
      "+------------+-----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Union\n",
    "=====\n",
    "'''\n",
    "\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/union/iris_union1.csv',sep=',',header=True)\n",
    "iris1_df2 = spark.read.csv(path='dataset/iris/union/iris_union2.csv',sep=',',header=True)\n",
    "iris1_df1.union(iris1_df2).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width', 'Species']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Retrive Column Names\n",
    "====================\n",
    "'''\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "iris1_df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Sepal_Length,StringType,true),StructField(Sepal_Width,StringType,true),StructField(Petal_Length,StringType,true),StructField(Petal_Width,StringType,true),StructField(Species,StringType,true)))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Retrieve Schema of the Data Frame\n",
    "=================================\n",
    "Structure for data frame can be defined with the help of StructField and \n",
    "StructType function. StructType is the data type representing a Row. \n",
    "It consisting of a list of StructField. StructField is a field in StructType. It's arguments are\n",
    "\n",
    "name - name of the columns\n",
    "datatype - data type of the column\n",
    "nullable - boolean value defining if the column is nullable or not\n",
    "'''\n",
    "\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "iris1_df1.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sepal_Length', 'string'),\n",
       " ('Sepal_Width', 'string'),\n",
       " ('Petal_Length', 'string'),\n",
       " ('Petal_Width', 'string'),\n",
       " ('Species', 'string')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Datatype\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "iris1_df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+------+\n",
      "| _1| _2| _3| _4|    _5|\n",
      "+---+---+---+---+------+\n",
      "|5.1|3.5|1.4|0.2|setosa|\n",
      "|4.9|3.0|1.4|0.2|setosa|\n",
      "|4.7|3.2|1.3|0.2|setosa|\n",
      "|4.6|3.1|1.5|0.2|setosa|\n",
      "|5.0|3.6|1.4|0.2|setosa|\n",
      "|5.4|3.9|1.7|0.4|setosa|\n",
      "|4.6|3.4|1.4|0.3|setosa|\n",
      "|5.0|3.4|1.5|0.2|setosa|\n",
      "|4.4|2.9|1.4|0.2|setosa|\n",
      "|4.9|3.1|1.5|0.1|setosa|\n",
      "|5.4|3.7|1.5|0.2|setosa|\n",
      "|4.8|3.4|1.6|0.2|setosa|\n",
      "|4.8|3.0|1.4|0.1|setosa|\n",
      "|4.3|3.0|1.1|0.1|setosa|\n",
      "|5.8|4.0|1.2|0.2|setosa|\n",
      "|5.7|4.4|1.5|0.4|setosa|\n",
      "|5.4|3.9|1.3|0.4|setosa|\n",
      "|5.1|3.5|1.4|0.3|setosa|\n",
      "|5.7|3.8|1.7|0.3|setosa|\n",
      "|5.1|3.8|1.5|0.3|setosa|\n",
      "+---+---+---+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Default Structure of Data Frame\n",
    "'''\n",
    "When the data frame is created from an RDD it can be observed from the below\n",
    "result that the Data Frame has no column header.\n",
    "In addition, when data is being imported from a csv file, there might be situations\n",
    "when a float column is defined as a string column. To overcome this issue, a structure\n",
    "needs to impose on the data to be imported.\n",
    "\n",
    "We will see how a structure can be imposed on the data frame.\n",
    "'''\n",
    "\n",
    "iris1 = sc.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda line: line.split(\",\"))\n",
    "iris1_split = iris1_split.map(lambda var1: [float(var1[0]), float(var1[1]), float(var1[2]), float(var1[3]), var1[4]])\n",
    "df1=spark.createDataFrame(iris1_split)\n",
    "df1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(Sepal_Length,FloatType,true),StructField(Sepal_Width,FloatType,true),StructField(Petal_Length,FloatType,true),StructField(Petal_Width,FloatType,true),StructField(Species,StringType,true)))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Defining Structure for Data Frame\n",
    "==================================\n",
    "A table contains multiple fields. So, while defining the structure of a table we need to define\n",
    " -> number of columns\n",
    " -> name of each column\n",
    " -> data type of each column\n",
    "\n",
    "For a structured data, all the rows have the same structure. Here, we define the structure\n",
    "of the row using StructType class. It takes as an argument, a collection of StructField \n",
    "class objects, which is used to define the metadata about the columns in each row.\n",
    "\n",
    "StructField takes as input\n",
    " -> name - name of the columns\n",
    " -> datatype - data type of the column\n",
    " -> nullable - boolean value defining if the column is nullable or not\n",
    "'''\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType\n",
    "iris_schema = pyspark.sql.types.StructType([\n",
    "StructField(\"Sepal_Length\", FloatType(), True),\n",
    "StructField(\"Sepal_Width\", FloatType(), True),\n",
    "StructField(\"Petal_Length\", FloatType(), True),\n",
    "StructField(\"Petal_Width\", FloatType(), True),\n",
    "StructField(\"Species\", StringType(), True)\n",
    "])\n",
    "print(iris_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Assigning Defined Structure to Data Frame\n",
    "=========================================\n",
    "\n",
    "When the data frame is created using an RDD, the defined schema can be\n",
    "assigned as highlighted in the below code snippet.\n",
    "'''\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType\n",
    "iris_schema = pyspark.sql.types.StructType([\n",
    "StructField(\"Sepal_Length\", FloatType(), True),\n",
    "StructField(\"Sepal_Width\", FloatType(), True),\n",
    "StructField(\"Petal_Length\", FloatType(), True),\n",
    "StructField(\"Petal_Width\", FloatType(), True),\n",
    "StructField(\"Species\", StringType(), True)\n",
    "])\n",
    "iris1 = sc.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda line: line.split(\",\"))\n",
    "iris1_split = iris1_split.map(lambda var1: [float(var1[0]), float(var1[1]), float(var1[2]), float(var1[3]),var1[4]])\n",
    "df1=spark.createDataFrame(iris1_split,iris_schema)\n",
    "df1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Importing with Schema\n",
    "======================\n",
    "\n",
    "'''\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType\n",
    "iris_schema = pyspark.sql.types.StructType([\n",
    "StructField(\"Sepal_Length\", FloatType(), True),\n",
    "StructField(\"Sepal_Width\", FloatType(), True),\n",
    "StructField(\"Petal_Length\", FloatType(), True),\n",
    "StructField(\"Petal_Width\", FloatType(), True),\n",
    "StructField(\"Species\", StringType(), True)\n",
    "])\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True,schema=iris_schema)\n",
    "iris1_df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Converting Datatype\n",
    "===================\n",
    "Data type of a particular column can be changed by first selecting those\n",
    "columns using select function and then changing its type using the cast function.\n",
    "'''\n",
    "\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "iris1_df1.dtypes\n",
    "iris1_df1.select(iris1_df1.Petal_Length.cast(\"float\"),iris1_df1.Sepal_Length.cast(\"float\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Drop Columns\n",
    "============\n",
    "\n",
    "'''\n",
    "\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "iris1_df1.drop('Species').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Drop by Selection\n",
    "=================\n",
    "Any particular column can be removed not only by using drop function, \n",
    "but also by selecting only the other required columns, other than the ones to be removed.\n",
    "'''\n",
    "\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "iris1_df1.select(\"Sepal_Length\",\"Species\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Descriptive Statistics on Data Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType\n",
    "iris_schema = pyspark.sql.types.StructType([\n",
    "StructField(\"Sepal_Length\", FloatType(), True),\n",
    "StructField(\"Sepal_Width\", FloatType(), True),\n",
    "StructField(\"Petal_Length\", FloatType(), True),\n",
    "StructField(\"Petal_Width\", FloatType(), True),\n",
    "StructField(\"Species\", StringType(), True)\n",
    "])\n",
    "\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True,schema=iris_schema)\n",
    "iris1_df1.sort('Petal_Length',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cannot sort directly after reading the file as csv\n",
    "## Datatype is not identified\n",
    "## need to map data types to column first to sort\n",
    "## when sort is applied directly to DF the o/p is wrong\n",
    "\n",
    "buy1 = spark.read.csv(path='dataset/iris/buy.csv',sep=',',header=True)\n",
    "buy1.sort('income', ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType\n",
    "buy_schema = pyspark.sql.types.StructType([\n",
    "StructField(\"age\", FloatType(), True),\n",
    "StructField(\"income\", FloatType(), True),\n",
    "StructField(\"gender\", StringType(), True),\n",
    "StructField(\"marital\", StringType(), True),\n",
    "StructField(\"buys\", StringType(), True)\n",
    "])\n",
    "\n",
    "buy_df1 = spark.read.csv(path='dataset/iris/buy.csv',sep=',',header=True, schema=buy_schema)\n",
    "\n",
    "buy_df1.sort(\"income\", ascending=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering Data Based on a Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "iris1_df1.select(\"Sepal_Length\",\"Species\").filter(\"Species=='virginica'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To filter only the flowers where 'Species' is either 'setosa' or 'versicolor'\n",
    "# isin function can be used as shown below.\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "iris1_df1[iris1_df1.Species.isin(['setosa'])].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distinct Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distinct\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "iris1_df1.select(\"Species\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distinct Count\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "iris1_df1.select(\"Species\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Aggregation\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "iris1_df1.agg({\"Sepal_Length\": \"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating Grouped Data\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "iris1_df1.groupBy('Species').agg({'Sepal_Length':'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Summary\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "iris1_df1.describe(['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width','Species']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Quantiles of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType\n",
    "iris_schema = pyspark.sql.types.StructType([\n",
    "StructField(\"Sepal_Length\", FloatType(), True),\n",
    "StructField(\"Sepal_Width\", FloatType(), True),\n",
    "StructField(\"Petal_Length\", FloatType(), True),\n",
    "StructField(\"Petal_Width\", FloatType(), True),\n",
    "StructField(\"Species\", StringType(), True)\n",
    "])\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True,schema=iris_schema)\n",
    "iris1_df1.approxQuantile( col='Sepal_Length', probabilities=[0.4, 0.6, 0.8], relativeError=0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Dimension View of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A multi-dimensional view of the data can be obtained with the help of cube function.\n",
    "In the example shown below, a multi-dimensional report is generated for the mean of\n",
    "'Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width' values for various Species.\n",
    "'''\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType\n",
    "iris_schema = pyspark.sql.types.StructType([\n",
    "StructField(\"Sepal_Length\", FloatType(), True),\n",
    "StructField(\"Sepal_Width\", FloatType(), True),\n",
    "StructField(\"Petal_Length\", FloatType(), True),\n",
    "StructField(\"Petal_Width\", FloatType(), True),\n",
    "StructField(\"Species\", StringType(), True)\n",
    "])\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True,schema=iris_schema)\n",
    "iris1_df1.cube('Species').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Co-variance and Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType\n",
    "iris_schema = pyspark.sql.types.StructType([\n",
    "StructField(\"Sepal_Length\", FloatType(), True),\n",
    "StructField(\"Sepal_Width\", FloatType(), True),\n",
    "StructField(\"Petal_Length\", FloatType(), True),\n",
    "StructField(\"Petal_Width\", FloatType(), True),\n",
    "StructField(\"Species\", StringType(), True)\n",
    "])\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True,schema=iris_schema)\n",
    "print('co-variance - ' , iris1_df1.cov('Sepal_Length','Petal_Length'))\n",
    "print('correlation - ' , iris1_df1.corr('Sepal_Length','Petal_Length'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType\n",
    "iris_schema = pyspark.sql.types.StructType([\n",
    "StructField(\"Sepal_Length\", FloatType(), True),\n",
    "StructField(\"Sepal_Width\", FloatType(), True),\n",
    "StructField(\"Petal_Length\", FloatType(), True),\n",
    "StructField(\"Petal_Width\", FloatType(), True),\n",
    "StructField(\"Species\", StringType(), True)\n",
    "])\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True,schema=iris_schema)\n",
    "iris1_df1.crosstab('Species','Species').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Temp Table from Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Temp tables can be created from an existing data frame using 'createOrReplaceTempView'\n",
    "or 'registerDataFrameAsTable' functions. In the below shown examples, temp table are \n",
    "created with the name 'iris1_table1', using these functions.\n",
    "'''\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "iris1_df1.createOrReplaceTempView(\"iris1_table1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "sqlContext.registerDataFrameAsTable(iris1_df1, \"iris1_table1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying Temp Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "iris1_df1.createOrReplaceTempView(\"iris1_table1\")\n",
    "sql1 = spark.sql(\"SELECT Sepal_Length, Species from iris1_table1\")\n",
    "sql1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve Available Tables in a Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tableNames function can be used to get a list of all the tables in a particular database. \n",
    "The name of the database is given as an argument to the tableNames function. \n",
    "If no name is given, then all the tables in the default database are printed\n",
    "'''\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "sqlContext.registerDataFrameAsTable(iris1_df1, \"iris1_table1\")\n",
    "sqlContext.tableNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Another method of retrieving the name of tables in a database is using 'tables' function\n",
    "\n",
    "This function returns a data frame containing the name of the database, table and the \n",
    "information if that particular table is temporary or not\n",
    "'''\n",
    "sqlContext.tables().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Table Back to Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A table can be converted back to a data frame using the table function\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "iris1_df1.createOrReplaceTempView(\"iris1_table1\")\n",
    "df1 = spark.table(\"iris1_table1\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive Intergration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Hive Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "As of version 2.0 of Spark, Hive tables can be accessing using Spark Context itself, \n",
    "and also the use of Hive context is deprecated. However, we will see how we can create one.\n",
    "\n",
    "Hive Context can be created using 'HiveContext' class. To HiveContext, \n",
    "we should pass asargument, the current Spark Context, as shown below.\n",
    "'''\n",
    "from pyspark.sql import HiveContext\n",
    "hc = HiveContext(sc)\n",
    "print(hc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing Hive Tables\n",
    "\n",
    "Hive data can be accessed using sqlContext or HiveContext. In this example, we will make use of sqlContext to access 'retailer' table present in 'db' database in Hive.\n",
    "The first step is to select the Hive database which contains our Hive table to be queried.\n",
    "###### sqlContext.sql('use db')\n",
    "\n",
    "Once selected, we can get a list of the tables available in this database using the 'tablesNames' data frame function.\n",
    "###### sqlContext.tableNames()\n",
    "\n",
    "Hive tables can be queried using HiveQL commands furnished using sql function, shown below.\n",
    "###### sqlContext.sql('select * from retailer').show()\n",
    "\n",
    "\n",
    "#### Exporting Data Frame To Hive\n",
    "\n",
    "Spark is meant to process the huge amount of data distributed among various system. The result of such large-scale processing will usually be very big and hence would be required to be stored on distributed systems. In this example, we will store the contents of a Spark Data Frame (iris dataset) on a Hadoop Distributed File System as a Hive table.\n",
    "\n",
    "To write data frame contents to Hive table we require 'saveAsTable' which is part of 'DataFrameWriter' class. The 'DataFrameWriter' class constructor takes as an argument, the data frame to be exported. The 'saveAsTable' function takes two arguments -\n",
    "name - name of the target table in Hive database\n",
    "mode - \n",
    "append: Append contents of the Data Frame to existing Hive table\n",
    "overwrite: Overwrite existing Hive table\n",
    "ignore: Silently ignore this operation if Hive table already exists\n",
    "error: Throw an exception if Hive table already exists\n",
    "\n",
    "It is first required to select the database where the data is to be written, then make use of 'DataFrameWriter' and 'saveAsTable' to export the data, as shown above.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType\n",
    "iris_schema = pyspark.sql.types.StructType([\n",
    "StructField(\"Sepal_Length\", FloatType(), True),\n",
    "StructField(\"Sepal_Width\", FloatType(), True),\n",
    "StructField(\"Petal_Length\", FloatType(), True),\n",
    "StructField(\"Petal_Width\", FloatType(), True),\n",
    "StructField(\"Species\", StringType(), True)\n",
    "])\n",
    "iris1_df1 = spark.read.csv(path='./dataset/iris.csv',sep=',',header=True,schema=iris_schema)\n",
    "sqlContext.sql('use db')\n",
    "from pyspark.sql import DataFrameWriter\n",
    "dfw = DataFrameWriter(iris1_df1)\n",
    "dfw.saveAsTable(name=\"iris\",mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to User-Define Functions\n",
    "\n",
    "Consider a scenario where a calculated column needs to be created in 10 Hive tables by performing complex operations on one of the existing columns in all of those 10 tables. A user-defined function can be created which performs the operation on its input data using sql and return the result. \n",
    "\n",
    "Python functions can be registered as Spark User-Defined Functions for two purposes.\n",
    "\n",
    "1. For being used in Spark SQL statements\n",
    "2. For being used on Spark Data Frame\n",
    "\n",
    "Both the methods would be discussed in order in the latter sections of this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UDF for SQL Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For a python function to be used in SQL queries, it needs to be registered using the 'register' function which is part of 'UDFRegistration' class.\n",
    "'UDFRegistration' takes sqlContext as an argument.\n",
    "To create UDF, we pass the following arguments\n",
    "\n",
    " -> name - name of the UDF\n",
    " -> f - python function to be registered as UDF\n",
    " -> returnType - data type of python function return value\n",
    "\n",
    "Below is an example to create a UDF with name 'new_fun' to adds an integer 10 to its input argument.\n",
    "'''\n",
    "\n",
    "\"\"\"\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql import UDFRegistration\n",
    "udf1 = UDFRegistration(sqlContext)\n",
    "udf1.register(name='new_fun', f=lambda var1: var1+10, returnType=FloatType())\n",
    "\n",
    "# UDF on Temp Tables\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType\n",
    "iris_schema = pyspark.sql.types.StructType([\n",
    "StructField(\"Sepal_Length\", FloatType(), True),\n",
    "StructField(\"Sepal_Width\", FloatType(), True),\n",
    "StructField(\"Petal_Length\", FloatType(), True),\n",
    "StructField(\"Petal_Width\", FloatType(), True),\n",
    "StructField(\"Species\", StringType(), True)\n",
    "])\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True,schema=iris_schema)\n",
    "sqlContext.registerDataFrameAsTable(iris1_df1, \"iris1_table1\")\n",
    "sqlContext.sql('select new_fun(Sepal_Length) from iris_temp').show()\n",
    "\n",
    "# UDF on Hive Tables\n",
    "\n",
    "sqlContext.sql('use db')\n",
    "sqlContext.sql('SELECT new_fun(Sepal_Length) FROM iris').show()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UDF for Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data Frame, UDF can be created using 'udf' function in 'pyspark.sql.functions' module. It takes two arguments.\n",
    " -> f – python function\n",
    " -> returnType – a pyspark.sql.types.DataType object\n",
    "Below is an example, to create a UDF with name 'new_fun' to add an integer 10 to its input argument.\n",
    "'''\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "new_fun2 = udf(lambda var1: var1+10, returnType=FloatType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType\n",
    "iris_schema = pyspark.sql.types.StructType([\n",
    "StructField(\"Sepal_Length\", FloatType(), True),\n",
    "StructField(\"Sepal_Width\", FloatType(), True),\n",
    "StructField(\"Petal_Length\", FloatType(), True),\n",
    "StructField(\"Petal_Width\", FloatType(), True),\n",
    "StructField(\"Species\", StringType(), True)\n",
    "])\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True,schema=iris_schema)\n",
    "iris1_df1.select(new_fun2(iris1_df1.Sepal_Length)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Iterative Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching is used to place the data in-memory, for fast computation of iterative processes. Data Frame, Temp Tables, and RDD can be cached. It works differently for tables and data frames. In case of tables, caching places the whole table in-memory. However, in data frames caching done is lazy, which mean that it will cache only those rows used in the forthcoming processing events. So, if we cache a particular data frame, it doesn't get in-memory immediately, instead, it will cache only the subset of that data fame on which some 'Action' has been performed post caching. So, for example, if a particular RDD has been cached, and only 50% of the data in that RDD is used in an Action, then only that 50% data of that RDD will be cached.\n",
    "\n",
    "Similar to cache, there exist something called persist, which is also used to enhance the execution performance of iterative operations. However, persist is capable of placing the iterative data only locations other than 'in-memory', like disk. Caching is nothing but persist 'in-memory only'. So, if it is required to assign another storage, then we can go ahead with the persist function.\n",
    "\n",
    "This section has examples to show how to cache, uncache, persist and unpersist on RDDs, Data Frames and Temp Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cache Uncache Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Cache\n",
    "=====\n",
    "RDDs and Dataframes can be cached using cache function, and tables can be cached using cacheTable function\n",
    "'''\n",
    "#RDD\n",
    "\n",
    "iris1 = sc.textFile(\"dataset/iris/iris_site.csv\")\n",
    "iris1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe cache\n",
    "\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "iris1_df1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table Cache\n",
    "\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "iris1_df1.createOrReplaceTempView(\"TempView1\")\n",
    "cache1 = sqlContext.cacheTable(\"TempView1\")\n",
    "cache1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cached tables can be uncached using 'uncacheTable' function as shown below.\n",
    "iris1_df1 = spark.read.csv(path='dataset/iris/iris.csv',sep=',',header=True)\n",
    "iris1_df1.createOrReplaceTempView(\"TempView1\")\n",
    "cache1 = sqlContext.cacheTable(\"TempView1\")\n",
    "sqlContext.uncacheTable(\"TempView1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, streaming\n",
    "str1 = streaming.StreamingContext(sparkContext=sc, batchDuration=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#socketTextStream(hostname, port, storageLevel=StorageLevel(True, True, False, False, 2))\n",
    "data1 = str1.socketTextStream(hostname=\"localhost\", port=8765)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.saveAsTextFiles('streaming/file')\n",
    "data1.pprint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1.start()\n",
    "str1.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-submit test1.py localhost 8765\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc -lk 8765\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
